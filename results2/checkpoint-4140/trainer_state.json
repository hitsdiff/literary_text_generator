{
  "best_metric": 1.8147141933441162,
  "best_model_checkpoint": "./results2/checkpoint-4140",
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 4140,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018115942028985508,
      "grad_norm": 2.975412607192993,
      "learning_rate": 4.9939613526570054e-05,
      "loss": 2.1082,
      "step": 5
    },
    {
      "epoch": 0.036231884057971016,
      "grad_norm": 3.850731134414673,
      "learning_rate": 4.98792270531401e-05,
      "loss": 2.142,
      "step": 10
    },
    {
      "epoch": 0.05434782608695652,
      "grad_norm": 3.9429831504821777,
      "learning_rate": 4.981884057971015e-05,
      "loss": 2.1821,
      "step": 15
    },
    {
      "epoch": 0.07246376811594203,
      "grad_norm": 4.119272708892822,
      "learning_rate": 4.9758454106280194e-05,
      "loss": 2.1576,
      "step": 20
    },
    {
      "epoch": 0.09057971014492754,
      "grad_norm": 3.7572543621063232,
      "learning_rate": 4.9698067632850245e-05,
      "loss": 2.1287,
      "step": 25
    },
    {
      "epoch": 0.10869565217391304,
      "grad_norm": 3.140639305114746,
      "learning_rate": 4.963768115942029e-05,
      "loss": 2.1422,
      "step": 30
    },
    {
      "epoch": 0.12681159420289856,
      "grad_norm": 3.6516618728637695,
      "learning_rate": 4.957729468599034e-05,
      "loss": 2.1429,
      "step": 35
    },
    {
      "epoch": 0.14492753623188406,
      "grad_norm": 4.560483932495117,
      "learning_rate": 4.9516908212560386e-05,
      "loss": 2.0939,
      "step": 40
    },
    {
      "epoch": 0.16304347826086957,
      "grad_norm": 3.437805652618408,
      "learning_rate": 4.945652173913044e-05,
      "loss": 2.1898,
      "step": 45
    },
    {
      "epoch": 0.18115942028985507,
      "grad_norm": 3.732732057571411,
      "learning_rate": 4.939613526570048e-05,
      "loss": 2.0222,
      "step": 50
    },
    {
      "epoch": 0.19927536231884058,
      "grad_norm": 5.064892768859863,
      "learning_rate": 4.933574879227053e-05,
      "loss": 2.0673,
      "step": 55
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 4.227930068969727,
      "learning_rate": 4.9275362318840584e-05,
      "loss": 1.8809,
      "step": 60
    },
    {
      "epoch": 0.23550724637681159,
      "grad_norm": 4.648601055145264,
      "learning_rate": 4.9214975845410636e-05,
      "loss": 2.1289,
      "step": 65
    },
    {
      "epoch": 0.2536231884057971,
      "grad_norm": 3.2281229496002197,
      "learning_rate": 4.915458937198068e-05,
      "loss": 2.1112,
      "step": 70
    },
    {
      "epoch": 0.2717391304347826,
      "grad_norm": 3.1938254833221436,
      "learning_rate": 4.909420289855073e-05,
      "loss": 2.1037,
      "step": 75
    },
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 3.5617146492004395,
      "learning_rate": 4.9033816425120776e-05,
      "loss": 2.1342,
      "step": 80
    },
    {
      "epoch": 0.3079710144927536,
      "grad_norm": 3.201178550720215,
      "learning_rate": 4.897342995169083e-05,
      "loss": 2.11,
      "step": 85
    },
    {
      "epoch": 0.32608695652173914,
      "grad_norm": 2.7665176391601562,
      "learning_rate": 4.891304347826087e-05,
      "loss": 2.0541,
      "step": 90
    },
    {
      "epoch": 0.3442028985507246,
      "grad_norm": 3.9637629985809326,
      "learning_rate": 4.885265700483092e-05,
      "loss": 2.108,
      "step": 95
    },
    {
      "epoch": 0.36231884057971014,
      "grad_norm": 3.470529794692993,
      "learning_rate": 4.879227053140097e-05,
      "loss": 2.0489,
      "step": 100
    },
    {
      "epoch": 0.3804347826086957,
      "grad_norm": 3.4395456314086914,
      "learning_rate": 4.873188405797102e-05,
      "loss": 2.1154,
      "step": 105
    },
    {
      "epoch": 0.39855072463768115,
      "grad_norm": 3.225494384765625,
      "learning_rate": 4.8671497584541064e-05,
      "loss": 2.1383,
      "step": 110
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 3.804119825363159,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 2.0598,
      "step": 115
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 4.707822322845459,
      "learning_rate": 4.855072463768116e-05,
      "loss": 2.0507,
      "step": 120
    },
    {
      "epoch": 0.4528985507246377,
      "grad_norm": 4.5991363525390625,
      "learning_rate": 4.849033816425121e-05,
      "loss": 2.0627,
      "step": 125
    },
    {
      "epoch": 0.47101449275362317,
      "grad_norm": 3.9230308532714844,
      "learning_rate": 4.8429951690821256e-05,
      "loss": 2.0989,
      "step": 130
    },
    {
      "epoch": 0.4891304347826087,
      "grad_norm": 3.595259189605713,
      "learning_rate": 4.836956521739131e-05,
      "loss": 2.1021,
      "step": 135
    },
    {
      "epoch": 0.5072463768115942,
      "grad_norm": 3.161172389984131,
      "learning_rate": 4.830917874396135e-05,
      "loss": 2.0861,
      "step": 140
    },
    {
      "epoch": 0.5253623188405797,
      "grad_norm": 3.53432559967041,
      "learning_rate": 4.82487922705314e-05,
      "loss": 2.0913,
      "step": 145
    },
    {
      "epoch": 0.5434782608695652,
      "grad_norm": 3.0825486183166504,
      "learning_rate": 4.818840579710145e-05,
      "loss": 2.1074,
      "step": 150
    },
    {
      "epoch": 0.5615942028985508,
      "grad_norm": 3.3211984634399414,
      "learning_rate": 4.81280193236715e-05,
      "loss": 2.0823,
      "step": 155
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 3.2257304191589355,
      "learning_rate": 4.806763285024155e-05,
      "loss": 2.1013,
      "step": 160
    },
    {
      "epoch": 0.5978260869565217,
      "grad_norm": 3.6807286739349365,
      "learning_rate": 4.80072463768116e-05,
      "loss": 2.1468,
      "step": 165
    },
    {
      "epoch": 0.6159420289855072,
      "grad_norm": 3.8097169399261475,
      "learning_rate": 4.7946859903381646e-05,
      "loss": 2.0838,
      "step": 170
    },
    {
      "epoch": 0.6340579710144928,
      "grad_norm": 3.6802496910095215,
      "learning_rate": 4.78864734299517e-05,
      "loss": 2.1476,
      "step": 175
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 3.650904417037964,
      "learning_rate": 4.782608695652174e-05,
      "loss": 2.0545,
      "step": 180
    },
    {
      "epoch": 0.6702898550724637,
      "grad_norm": 3.014678716659546,
      "learning_rate": 4.776570048309179e-05,
      "loss": 2.0783,
      "step": 185
    },
    {
      "epoch": 0.6884057971014492,
      "grad_norm": 3.2072746753692627,
      "learning_rate": 4.770531400966184e-05,
      "loss": 2.0944,
      "step": 190
    },
    {
      "epoch": 0.7065217391304348,
      "grad_norm": 3.6258227825164795,
      "learning_rate": 4.764492753623189e-05,
      "loss": 2.0901,
      "step": 195
    },
    {
      "epoch": 0.7246376811594203,
      "grad_norm": 3.539940118789673,
      "learning_rate": 4.7584541062801933e-05,
      "loss": 2.089,
      "step": 200
    },
    {
      "epoch": 0.7427536231884058,
      "grad_norm": 2.9780616760253906,
      "learning_rate": 4.7524154589371985e-05,
      "loss": 2.0919,
      "step": 205
    },
    {
      "epoch": 0.7608695652173914,
      "grad_norm": 3.3333873748779297,
      "learning_rate": 4.746376811594203e-05,
      "loss": 2.0838,
      "step": 210
    },
    {
      "epoch": 0.7789855072463768,
      "grad_norm": 3.433832883834839,
      "learning_rate": 4.740338164251208e-05,
      "loss": 2.1392,
      "step": 215
    },
    {
      "epoch": 0.7971014492753623,
      "grad_norm": 3.61356258392334,
      "learning_rate": 4.7342995169082125e-05,
      "loss": 2.0513,
      "step": 220
    },
    {
      "epoch": 0.8152173913043478,
      "grad_norm": 3.3889353275299072,
      "learning_rate": 4.7282608695652177e-05,
      "loss": 1.9981,
      "step": 225
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 3.488780975341797,
      "learning_rate": 4.722222222222222e-05,
      "loss": 2.0306,
      "step": 230
    },
    {
      "epoch": 0.8514492753623188,
      "grad_norm": 3.508096694946289,
      "learning_rate": 4.716183574879227e-05,
      "loss": 2.0947,
      "step": 235
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 5.06129789352417,
      "learning_rate": 4.710144927536232e-05,
      "loss": 2.085,
      "step": 240
    },
    {
      "epoch": 0.8876811594202898,
      "grad_norm": 3.71720552444458,
      "learning_rate": 4.704106280193237e-05,
      "loss": 2.1072,
      "step": 245
    },
    {
      "epoch": 0.9057971014492754,
      "grad_norm": 3.2671022415161133,
      "learning_rate": 4.698067632850241e-05,
      "loss": 2.0875,
      "step": 250
    },
    {
      "epoch": 0.9239130434782609,
      "grad_norm": 3.36301589012146,
      "learning_rate": 4.6920289855072464e-05,
      "loss": 2.0281,
      "step": 255
    },
    {
      "epoch": 0.9420289855072463,
      "grad_norm": 4.371739387512207,
      "learning_rate": 4.6859903381642516e-05,
      "loss": 2.0626,
      "step": 260
    },
    {
      "epoch": 0.9601449275362319,
      "grad_norm": 3.405752420425415,
      "learning_rate": 4.679951690821257e-05,
      "loss": 2.1105,
      "step": 265
    },
    {
      "epoch": 0.9782608695652174,
      "grad_norm": 3.811004161834717,
      "learning_rate": 4.673913043478261e-05,
      "loss": 2.0473,
      "step": 270
    },
    {
      "epoch": 0.9963768115942029,
      "grad_norm": 4.243504047393799,
      "learning_rate": 4.667874396135266e-05,
      "loss": 2.087,
      "step": 275
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9827351570129395,
      "eval_runtime": 3.1305,
      "eval_samples_per_second": 15.652,
      "eval_steps_per_second": 2.236,
      "step": 276
    },
    {
      "epoch": 1.0144927536231885,
      "grad_norm": 3.446493625640869,
      "learning_rate": 4.661835748792271e-05,
      "loss": 2.1037,
      "step": 280
    },
    {
      "epoch": 1.0326086956521738,
      "grad_norm": 3.866110324859619,
      "learning_rate": 4.655797101449276e-05,
      "loss": 2.0655,
      "step": 285
    },
    {
      "epoch": 1.0507246376811594,
      "grad_norm": 3.2822368144989014,
      "learning_rate": 4.64975845410628e-05,
      "loss": 1.9997,
      "step": 290
    },
    {
      "epoch": 1.068840579710145,
      "grad_norm": 3.7442729473114014,
      "learning_rate": 4.6437198067632854e-05,
      "loss": 2.0355,
      "step": 295
    },
    {
      "epoch": 1.0869565217391304,
      "grad_norm": 3.404496431350708,
      "learning_rate": 4.63768115942029e-05,
      "loss": 1.9515,
      "step": 300
    },
    {
      "epoch": 1.105072463768116,
      "grad_norm": 3.4328153133392334,
      "learning_rate": 4.631642512077295e-05,
      "loss": 2.0506,
      "step": 305
    },
    {
      "epoch": 1.1231884057971016,
      "grad_norm": 3.395228147506714,
      "learning_rate": 4.6256038647342995e-05,
      "loss": 2.0481,
      "step": 310
    },
    {
      "epoch": 1.141304347826087,
      "grad_norm": 3.337423324584961,
      "learning_rate": 4.6195652173913046e-05,
      "loss": 1.9803,
      "step": 315
    },
    {
      "epoch": 1.1594202898550725,
      "grad_norm": 4.451778888702393,
      "learning_rate": 4.613526570048309e-05,
      "loss": 2.0575,
      "step": 320
    },
    {
      "epoch": 1.177536231884058,
      "grad_norm": 4.250080108642578,
      "learning_rate": 4.607487922705314e-05,
      "loss": 2.0118,
      "step": 325
    },
    {
      "epoch": 1.1956521739130435,
      "grad_norm": 3.678277015686035,
      "learning_rate": 4.601449275362319e-05,
      "loss": 1.9982,
      "step": 330
    },
    {
      "epoch": 1.213768115942029,
      "grad_norm": 3.084256410598755,
      "learning_rate": 4.595410628019324e-05,
      "loss": 2.0505,
      "step": 335
    },
    {
      "epoch": 1.2318840579710144,
      "grad_norm": 3.9596943855285645,
      "learning_rate": 4.589371980676328e-05,
      "loss": 2.0552,
      "step": 340
    },
    {
      "epoch": 1.25,
      "grad_norm": 3.5988547801971436,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 2.0554,
      "step": 345
    },
    {
      "epoch": 1.2681159420289856,
      "grad_norm": 3.6399526596069336,
      "learning_rate": 4.577294685990338e-05,
      "loss": 1.9927,
      "step": 350
    },
    {
      "epoch": 1.286231884057971,
      "grad_norm": 3.387484073638916,
      "learning_rate": 4.571256038647343e-05,
      "loss": 2.0141,
      "step": 355
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 3.1842880249023438,
      "learning_rate": 4.565217391304348e-05,
      "loss": 2.0249,
      "step": 360
    },
    {
      "epoch": 1.322463768115942,
      "grad_norm": 3.3311562538146973,
      "learning_rate": 4.559178743961353e-05,
      "loss": 2.0931,
      "step": 365
    },
    {
      "epoch": 1.3405797101449275,
      "grad_norm": 2.7850019931793213,
      "learning_rate": 4.553140096618358e-05,
      "loss": 2.0408,
      "step": 370
    },
    {
      "epoch": 1.358695652173913,
      "grad_norm": 3.7570128440856934,
      "learning_rate": 4.547101449275363e-05,
      "loss": 2.0229,
      "step": 375
    },
    {
      "epoch": 1.3768115942028984,
      "grad_norm": 2.8022165298461914,
      "learning_rate": 4.541062801932367e-05,
      "loss": 2.0738,
      "step": 380
    },
    {
      "epoch": 1.394927536231884,
      "grad_norm": 2.8607213497161865,
      "learning_rate": 4.5350241545893724e-05,
      "loss": 2.0448,
      "step": 385
    },
    {
      "epoch": 1.4130434782608696,
      "grad_norm": 4.251960754394531,
      "learning_rate": 4.528985507246377e-05,
      "loss": 2.0785,
      "step": 390
    },
    {
      "epoch": 1.431159420289855,
      "grad_norm": 3.0261282920837402,
      "learning_rate": 4.522946859903382e-05,
      "loss": 2.0269,
      "step": 395
    },
    {
      "epoch": 1.4492753623188406,
      "grad_norm": 3.6424596309661865,
      "learning_rate": 4.5169082125603865e-05,
      "loss": 2.0195,
      "step": 400
    },
    {
      "epoch": 1.4673913043478262,
      "grad_norm": 2.869795083999634,
      "learning_rate": 4.5108695652173916e-05,
      "loss": 2.1027,
      "step": 405
    },
    {
      "epoch": 1.4855072463768115,
      "grad_norm": 4.009125709533691,
      "learning_rate": 4.504830917874396e-05,
      "loss": 2.005,
      "step": 410
    },
    {
      "epoch": 1.5036231884057971,
      "grad_norm": 3.2276010513305664,
      "learning_rate": 4.498792270531401e-05,
      "loss": 2.076,
      "step": 415
    },
    {
      "epoch": 1.5217391304347827,
      "grad_norm": 3.700803279876709,
      "learning_rate": 4.492753623188406e-05,
      "loss": 2.0323,
      "step": 420
    },
    {
      "epoch": 1.539855072463768,
      "grad_norm": 3.449157238006592,
      "learning_rate": 4.486714975845411e-05,
      "loss": 2.0752,
      "step": 425
    },
    {
      "epoch": 1.5579710144927537,
      "grad_norm": 3.214250087738037,
      "learning_rate": 4.480676328502416e-05,
      "loss": 2.0394,
      "step": 430
    },
    {
      "epoch": 1.5760869565217392,
      "grad_norm": 3.8705780506134033,
      "learning_rate": 4.4746376811594203e-05,
      "loss": 2.081,
      "step": 435
    },
    {
      "epoch": 1.5942028985507246,
      "grad_norm": 3.246216058731079,
      "learning_rate": 4.4685990338164255e-05,
      "loss": 1.9373,
      "step": 440
    },
    {
      "epoch": 1.6123188405797102,
      "grad_norm": 3.0199429988861084,
      "learning_rate": 4.46256038647343e-05,
      "loss": 2.0043,
      "step": 445
    },
    {
      "epoch": 1.6304347826086958,
      "grad_norm": 3.8408010005950928,
      "learning_rate": 4.456521739130435e-05,
      "loss": 1.9718,
      "step": 450
    },
    {
      "epoch": 1.6485507246376812,
      "grad_norm": 3.814016580581665,
      "learning_rate": 4.4504830917874395e-05,
      "loss": 1.765,
      "step": 455
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 3.1746749877929688,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 2.0519,
      "step": 460
    },
    {
      "epoch": 1.6847826086956523,
      "grad_norm": 3.3911468982696533,
      "learning_rate": 4.438405797101449e-05,
      "loss": 2.0219,
      "step": 465
    },
    {
      "epoch": 1.7028985507246377,
      "grad_norm": 3.121338367462158,
      "learning_rate": 4.432367149758454e-05,
      "loss": 1.9901,
      "step": 470
    },
    {
      "epoch": 1.721014492753623,
      "grad_norm": 3.8804049491882324,
      "learning_rate": 4.4263285024154594e-05,
      "loss": 1.9938,
      "step": 475
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 3.632854461669922,
      "learning_rate": 4.4202898550724645e-05,
      "loss": 2.0472,
      "step": 480
    },
    {
      "epoch": 1.7572463768115942,
      "grad_norm": 4.543435096740723,
      "learning_rate": 4.414251207729469e-05,
      "loss": 2.0691,
      "step": 485
    },
    {
      "epoch": 1.7753623188405796,
      "grad_norm": 3.159339189529419,
      "learning_rate": 4.408212560386474e-05,
      "loss": 2.0157,
      "step": 490
    },
    {
      "epoch": 1.7934782608695652,
      "grad_norm": 3.9662911891937256,
      "learning_rate": 4.4021739130434786e-05,
      "loss": 2.0611,
      "step": 495
    },
    {
      "epoch": 1.8115942028985508,
      "grad_norm": 3.4396300315856934,
      "learning_rate": 4.396135265700484e-05,
      "loss": 1.9816,
      "step": 500
    },
    {
      "epoch": 1.8297101449275361,
      "grad_norm": 2.9961049556732178,
      "learning_rate": 4.390096618357488e-05,
      "loss": 2.0456,
      "step": 505
    },
    {
      "epoch": 1.8478260869565217,
      "grad_norm": 3.355889320373535,
      "learning_rate": 4.384057971014493e-05,
      "loss": 2.007,
      "step": 510
    },
    {
      "epoch": 1.8659420289855073,
      "grad_norm": 2.9592108726501465,
      "learning_rate": 4.378019323671498e-05,
      "loss": 2.029,
      "step": 515
    },
    {
      "epoch": 1.8840579710144927,
      "grad_norm": 3.8434886932373047,
      "learning_rate": 4.371980676328503e-05,
      "loss": 2.1067,
      "step": 520
    },
    {
      "epoch": 1.9021739130434783,
      "grad_norm": 3.763369083404541,
      "learning_rate": 4.365942028985507e-05,
      "loss": 2.0451,
      "step": 525
    },
    {
      "epoch": 1.9202898550724639,
      "grad_norm": 2.986166477203369,
      "learning_rate": 4.3599033816425124e-05,
      "loss": 2.0441,
      "step": 530
    },
    {
      "epoch": 1.9384057971014492,
      "grad_norm": 3.7543601989746094,
      "learning_rate": 4.353864734299517e-05,
      "loss": 2.0548,
      "step": 535
    },
    {
      "epoch": 1.9565217391304348,
      "grad_norm": 3.230278968811035,
      "learning_rate": 4.347826086956522e-05,
      "loss": 2.0485,
      "step": 540
    },
    {
      "epoch": 1.9746376811594204,
      "grad_norm": 3.3492116928100586,
      "learning_rate": 4.3417874396135265e-05,
      "loss": 1.9703,
      "step": 545
    },
    {
      "epoch": 1.9927536231884058,
      "grad_norm": 3.0966577529907227,
      "learning_rate": 4.3357487922705316e-05,
      "loss": 1.9814,
      "step": 550
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9463809728622437,
      "eval_runtime": 2.4523,
      "eval_samples_per_second": 19.981,
      "eval_steps_per_second": 2.854,
      "step": 552
    },
    {
      "epoch": 2.010869565217391,
      "grad_norm": 2.595308780670166,
      "learning_rate": 4.329710144927536e-05,
      "loss": 2.0532,
      "step": 555
    },
    {
      "epoch": 2.028985507246377,
      "grad_norm": 4.352484703063965,
      "learning_rate": 4.323671497584541e-05,
      "loss": 2.0379,
      "step": 560
    },
    {
      "epoch": 2.0471014492753623,
      "grad_norm": 3.516932487487793,
      "learning_rate": 4.317632850241546e-05,
      "loss": 2.01,
      "step": 565
    },
    {
      "epoch": 2.0652173913043477,
      "grad_norm": 3.1859359741210938,
      "learning_rate": 4.3115942028985515e-05,
      "loss": 1.9871,
      "step": 570
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 3.5020406246185303,
      "learning_rate": 4.305555555555556e-05,
      "loss": 1.7005,
      "step": 575
    },
    {
      "epoch": 2.101449275362319,
      "grad_norm": 3.4378178119659424,
      "learning_rate": 4.299516908212561e-05,
      "loss": 1.9803,
      "step": 580
    },
    {
      "epoch": 2.119565217391304,
      "grad_norm": 2.7599499225616455,
      "learning_rate": 4.2934782608695655e-05,
      "loss": 1.9396,
      "step": 585
    },
    {
      "epoch": 2.13768115942029,
      "grad_norm": 3.574747323989868,
      "learning_rate": 4.2874396135265707e-05,
      "loss": 1.9825,
      "step": 590
    },
    {
      "epoch": 2.1557971014492754,
      "grad_norm": 3.66837477684021,
      "learning_rate": 4.281400966183575e-05,
      "loss": 1.9747,
      "step": 595
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 3.6527750492095947,
      "learning_rate": 4.27536231884058e-05,
      "loss": 1.9965,
      "step": 600
    },
    {
      "epoch": 2.1920289855072466,
      "grad_norm": 3.4248805046081543,
      "learning_rate": 4.269323671497585e-05,
      "loss": 1.9237,
      "step": 605
    },
    {
      "epoch": 2.210144927536232,
      "grad_norm": 3.227882146835327,
      "learning_rate": 4.26328502415459e-05,
      "loss": 2.0451,
      "step": 610
    },
    {
      "epoch": 2.2282608695652173,
      "grad_norm": 3.6167244911193848,
      "learning_rate": 4.257246376811594e-05,
      "loss": 1.9236,
      "step": 615
    },
    {
      "epoch": 2.246376811594203,
      "grad_norm": 3.5121428966522217,
      "learning_rate": 4.2512077294685994e-05,
      "loss": 1.9893,
      "step": 620
    },
    {
      "epoch": 2.2644927536231885,
      "grad_norm": 3.522942304611206,
      "learning_rate": 4.245169082125604e-05,
      "loss": 2.0465,
      "step": 625
    },
    {
      "epoch": 2.282608695652174,
      "grad_norm": 4.109868049621582,
      "learning_rate": 4.239130434782609e-05,
      "loss": 1.9996,
      "step": 630
    },
    {
      "epoch": 2.300724637681159,
      "grad_norm": 3.172626256942749,
      "learning_rate": 4.2330917874396135e-05,
      "loss": 2.0045,
      "step": 635
    },
    {
      "epoch": 2.318840579710145,
      "grad_norm": 2.777815580368042,
      "learning_rate": 4.2270531400966186e-05,
      "loss": 1.9886,
      "step": 640
    },
    {
      "epoch": 2.3369565217391304,
      "grad_norm": 2.8207991123199463,
      "learning_rate": 4.221014492753623e-05,
      "loss": 2.0346,
      "step": 645
    },
    {
      "epoch": 2.355072463768116,
      "grad_norm": 4.376982688903809,
      "learning_rate": 4.214975845410628e-05,
      "loss": 2.0291,
      "step": 650
    },
    {
      "epoch": 2.3731884057971016,
      "grad_norm": 3.8055834770202637,
      "learning_rate": 4.2089371980676326e-05,
      "loss": 1.9926,
      "step": 655
    },
    {
      "epoch": 2.391304347826087,
      "grad_norm": 3.4549946784973145,
      "learning_rate": 4.202898550724638e-05,
      "loss": 1.9242,
      "step": 660
    },
    {
      "epoch": 2.4094202898550723,
      "grad_norm": 3.3837099075317383,
      "learning_rate": 4.196859903381642e-05,
      "loss": 1.9906,
      "step": 665
    },
    {
      "epoch": 2.427536231884058,
      "grad_norm": 4.561999797821045,
      "learning_rate": 4.1908212560386474e-05,
      "loss": 1.9745,
      "step": 670
    },
    {
      "epoch": 2.4456521739130435,
      "grad_norm": 3.3319036960601807,
      "learning_rate": 4.1847826086956525e-05,
      "loss": 2.0306,
      "step": 675
    },
    {
      "epoch": 2.463768115942029,
      "grad_norm": 2.72925066947937,
      "learning_rate": 4.1787439613526576e-05,
      "loss": 2.0109,
      "step": 680
    },
    {
      "epoch": 2.4818840579710146,
      "grad_norm": 3.160187244415283,
      "learning_rate": 4.172705314009662e-05,
      "loss": 2.0073,
      "step": 685
    },
    {
      "epoch": 2.5,
      "grad_norm": 2.7048232555389404,
      "learning_rate": 4.166666666666667e-05,
      "loss": 2.0691,
      "step": 690
    },
    {
      "epoch": 2.5181159420289854,
      "grad_norm": 3.6437950134277344,
      "learning_rate": 4.1606280193236717e-05,
      "loss": 1.9343,
      "step": 695
    },
    {
      "epoch": 2.536231884057971,
      "grad_norm": 3.4773268699645996,
      "learning_rate": 4.154589371980677e-05,
      "loss": 1.9735,
      "step": 700
    },
    {
      "epoch": 2.5543478260869565,
      "grad_norm": 3.4947242736816406,
      "learning_rate": 4.148550724637681e-05,
      "loss": 2.0276,
      "step": 705
    },
    {
      "epoch": 2.572463768115942,
      "grad_norm": 3.4147517681121826,
      "learning_rate": 4.1425120772946864e-05,
      "loss": 2.0007,
      "step": 710
    },
    {
      "epoch": 2.5905797101449277,
      "grad_norm": 4.111979961395264,
      "learning_rate": 4.136473429951691e-05,
      "loss": 1.9675,
      "step": 715
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 2.8601958751678467,
      "learning_rate": 4.130434782608696e-05,
      "loss": 1.9332,
      "step": 720
    },
    {
      "epoch": 2.6268115942028984,
      "grad_norm": 3.7121427059173584,
      "learning_rate": 4.1243961352657004e-05,
      "loss": 1.9973,
      "step": 725
    },
    {
      "epoch": 2.644927536231884,
      "grad_norm": 3.7107138633728027,
      "learning_rate": 4.1183574879227056e-05,
      "loss": 1.9704,
      "step": 730
    },
    {
      "epoch": 2.6630434782608696,
      "grad_norm": 3.505107879638672,
      "learning_rate": 4.11231884057971e-05,
      "loss": 1.9375,
      "step": 735
    },
    {
      "epoch": 2.681159420289855,
      "grad_norm": 3.913898229598999,
      "learning_rate": 4.106280193236715e-05,
      "loss": 1.972,
      "step": 740
    },
    {
      "epoch": 2.699275362318841,
      "grad_norm": 3.4738926887512207,
      "learning_rate": 4.1002415458937196e-05,
      "loss": 2.0247,
      "step": 745
    },
    {
      "epoch": 2.717391304347826,
      "grad_norm": 4.547034740447998,
      "learning_rate": 4.094202898550725e-05,
      "loss": 2.0344,
      "step": 750
    },
    {
      "epoch": 2.7355072463768115,
      "grad_norm": 4.034386157989502,
      "learning_rate": 4.088164251207729e-05,
      "loss": 1.9561,
      "step": 755
    },
    {
      "epoch": 2.753623188405797,
      "grad_norm": 3.6130857467651367,
      "learning_rate": 4.082125603864734e-05,
      "loss": 1.9568,
      "step": 760
    },
    {
      "epoch": 2.7717391304347827,
      "grad_norm": 3.642184257507324,
      "learning_rate": 4.076086956521739e-05,
      "loss": 1.962,
      "step": 765
    },
    {
      "epoch": 2.789855072463768,
      "grad_norm": 2.408003568649292,
      "learning_rate": 4.070048309178744e-05,
      "loss": 1.9865,
      "step": 770
    },
    {
      "epoch": 2.807971014492754,
      "grad_norm": 4.219727516174316,
      "learning_rate": 4.064009661835749e-05,
      "loss": 1.9829,
      "step": 775
    },
    {
      "epoch": 2.8260869565217392,
      "grad_norm": 3.86338210105896,
      "learning_rate": 4.057971014492754e-05,
      "loss": 2.0076,
      "step": 780
    },
    {
      "epoch": 2.8442028985507246,
      "grad_norm": 3.7167179584503174,
      "learning_rate": 4.0519323671497586e-05,
      "loss": 1.9972,
      "step": 785
    },
    {
      "epoch": 2.86231884057971,
      "grad_norm": 3.5388715267181396,
      "learning_rate": 4.045893719806764e-05,
      "loss": 1.9984,
      "step": 790
    },
    {
      "epoch": 2.880434782608696,
      "grad_norm": 2.8969759941101074,
      "learning_rate": 4.039855072463768e-05,
      "loss": 1.9493,
      "step": 795
    },
    {
      "epoch": 2.898550724637681,
      "grad_norm": 2.9437546730041504,
      "learning_rate": 4.0338164251207733e-05,
      "loss": 2.0351,
      "step": 800
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 3.8697800636291504,
      "learning_rate": 4.027777777777778e-05,
      "loss": 1.9788,
      "step": 805
    },
    {
      "epoch": 2.9347826086956523,
      "grad_norm": 3.2649638652801514,
      "learning_rate": 4.021739130434783e-05,
      "loss": 2.0035,
      "step": 810
    },
    {
      "epoch": 2.9528985507246377,
      "grad_norm": 3.2882461547851562,
      "learning_rate": 4.0157004830917874e-05,
      "loss": 1.9731,
      "step": 815
    },
    {
      "epoch": 2.971014492753623,
      "grad_norm": 3.347141742706299,
      "learning_rate": 4.0096618357487925e-05,
      "loss": 2.0257,
      "step": 820
    },
    {
      "epoch": 2.9891304347826084,
      "grad_norm": 4.282526016235352,
      "learning_rate": 4.003623188405797e-05,
      "loss": 1.9619,
      "step": 825
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9239706993103027,
      "eval_runtime": 2.5158,
      "eval_samples_per_second": 19.477,
      "eval_steps_per_second": 2.782,
      "step": 828
    },
    {
      "epoch": 3.0072463768115942,
      "grad_norm": 3.544858694076538,
      "learning_rate": 3.997584541062802e-05,
      "loss": 1.9901,
      "step": 830
    },
    {
      "epoch": 3.0253623188405796,
      "grad_norm": 3.571087121963501,
      "learning_rate": 3.991545893719807e-05,
      "loss": 1.9597,
      "step": 835
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 3.82737135887146,
      "learning_rate": 3.985507246376812e-05,
      "loss": 1.9166,
      "step": 840
    },
    {
      "epoch": 3.0615942028985508,
      "grad_norm": 3.292070150375366,
      "learning_rate": 3.979468599033817e-05,
      "loss": 1.9814,
      "step": 845
    },
    {
      "epoch": 3.079710144927536,
      "grad_norm": 3.5561182498931885,
      "learning_rate": 3.973429951690821e-05,
      "loss": 1.953,
      "step": 850
    },
    {
      "epoch": 3.097826086956522,
      "grad_norm": 3.6392197608947754,
      "learning_rate": 3.9673913043478264e-05,
      "loss": 1.9692,
      "step": 855
    },
    {
      "epoch": 3.1159420289855073,
      "grad_norm": 3.2216694355010986,
      "learning_rate": 3.961352657004831e-05,
      "loss": 1.9566,
      "step": 860
    },
    {
      "epoch": 3.1340579710144927,
      "grad_norm": 3.3172719478607178,
      "learning_rate": 3.955314009661836e-05,
      "loss": 2.0102,
      "step": 865
    },
    {
      "epoch": 3.1521739130434785,
      "grad_norm": 2.903297185897827,
      "learning_rate": 3.9492753623188405e-05,
      "loss": 1.9596,
      "step": 870
    },
    {
      "epoch": 3.170289855072464,
      "grad_norm": 4.108119964599609,
      "learning_rate": 3.9432367149758456e-05,
      "loss": 2.0035,
      "step": 875
    },
    {
      "epoch": 3.1884057971014492,
      "grad_norm": 3.025317430496216,
      "learning_rate": 3.937198067632851e-05,
      "loss": 1.9626,
      "step": 880
    },
    {
      "epoch": 3.2065217391304346,
      "grad_norm": 3.1562814712524414,
      "learning_rate": 3.931159420289855e-05,
      "loss": 1.9343,
      "step": 885
    },
    {
      "epoch": 3.2246376811594204,
      "grad_norm": 3.2030959129333496,
      "learning_rate": 3.92512077294686e-05,
      "loss": 1.8712,
      "step": 890
    },
    {
      "epoch": 3.2427536231884058,
      "grad_norm": 3.006026268005371,
      "learning_rate": 3.9190821256038654e-05,
      "loss": 1.9402,
      "step": 895
    },
    {
      "epoch": 3.260869565217391,
      "grad_norm": 3.3590686321258545,
      "learning_rate": 3.91304347826087e-05,
      "loss": 1.8896,
      "step": 900
    },
    {
      "epoch": 3.278985507246377,
      "grad_norm": 3.316824436187744,
      "learning_rate": 3.907004830917875e-05,
      "loss": 1.9526,
      "step": 905
    },
    {
      "epoch": 3.2971014492753623,
      "grad_norm": 3.126034736633301,
      "learning_rate": 3.9009661835748795e-05,
      "loss": 1.9307,
      "step": 910
    },
    {
      "epoch": 3.3152173913043477,
      "grad_norm": 3.3687541484832764,
      "learning_rate": 3.8949275362318846e-05,
      "loss": 1.9736,
      "step": 915
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 3.4534335136413574,
      "learning_rate": 3.888888888888889e-05,
      "loss": 1.9623,
      "step": 920
    },
    {
      "epoch": 3.351449275362319,
      "grad_norm": 3.133970022201538,
      "learning_rate": 3.882850241545894e-05,
      "loss": 1.9922,
      "step": 925
    },
    {
      "epoch": 3.369565217391304,
      "grad_norm": 3.7541592121124268,
      "learning_rate": 3.876811594202899e-05,
      "loss": 1.9337,
      "step": 930
    },
    {
      "epoch": 3.38768115942029,
      "grad_norm": 3.53428053855896,
      "learning_rate": 3.870772946859904e-05,
      "loss": 1.9814,
      "step": 935
    },
    {
      "epoch": 3.4057971014492754,
      "grad_norm": 3.612917900085449,
      "learning_rate": 3.864734299516908e-05,
      "loss": 1.9508,
      "step": 940
    },
    {
      "epoch": 3.4239130434782608,
      "grad_norm": 3.299781560897827,
      "learning_rate": 3.8586956521739134e-05,
      "loss": 1.9744,
      "step": 945
    },
    {
      "epoch": 3.4420289855072466,
      "grad_norm": 3.0586507320404053,
      "learning_rate": 3.852657004830918e-05,
      "loss": 2.0045,
      "step": 950
    },
    {
      "epoch": 3.460144927536232,
      "grad_norm": 3.3352551460266113,
      "learning_rate": 3.846618357487923e-05,
      "loss": 1.9486,
      "step": 955
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 2.822741985321045,
      "learning_rate": 3.8405797101449274e-05,
      "loss": 1.9436,
      "step": 960
    },
    {
      "epoch": 3.496376811594203,
      "grad_norm": 4.2511701583862305,
      "learning_rate": 3.8345410628019326e-05,
      "loss": 1.9423,
      "step": 965
    },
    {
      "epoch": 3.5144927536231885,
      "grad_norm": 3.7497167587280273,
      "learning_rate": 3.828502415458937e-05,
      "loss": 2.0178,
      "step": 970
    },
    {
      "epoch": 3.532608695652174,
      "grad_norm": 3.231017827987671,
      "learning_rate": 3.822463768115942e-05,
      "loss": 1.9966,
      "step": 975
    },
    {
      "epoch": 3.550724637681159,
      "grad_norm": 4.33617639541626,
      "learning_rate": 3.8164251207729466e-05,
      "loss": 1.8917,
      "step": 980
    },
    {
      "epoch": 3.568840579710145,
      "grad_norm": 3.8504180908203125,
      "learning_rate": 3.8103864734299524e-05,
      "loss": 1.9566,
      "step": 985
    },
    {
      "epoch": 3.5869565217391304,
      "grad_norm": 4.046841144561768,
      "learning_rate": 3.804347826086957e-05,
      "loss": 1.8831,
      "step": 990
    },
    {
      "epoch": 3.605072463768116,
      "grad_norm": 3.6032073497772217,
      "learning_rate": 3.798309178743962e-05,
      "loss": 1.9476,
      "step": 995
    },
    {
      "epoch": 3.6231884057971016,
      "grad_norm": 2.8434016704559326,
      "learning_rate": 3.7922705314009665e-05,
      "loss": 1.9429,
      "step": 1000
    },
    {
      "epoch": 3.641304347826087,
      "grad_norm": 4.168697357177734,
      "learning_rate": 3.7862318840579716e-05,
      "loss": 1.954,
      "step": 1005
    },
    {
      "epoch": 3.6594202898550723,
      "grad_norm": 3.5477256774902344,
      "learning_rate": 3.780193236714976e-05,
      "loss": 1.9592,
      "step": 1010
    },
    {
      "epoch": 3.677536231884058,
      "grad_norm": 3.6439852714538574,
      "learning_rate": 3.774154589371981e-05,
      "loss": 1.9956,
      "step": 1015
    },
    {
      "epoch": 3.6956521739130435,
      "grad_norm": 4.1706624031066895,
      "learning_rate": 3.7681159420289856e-05,
      "loss": 1.9583,
      "step": 1020
    },
    {
      "epoch": 3.713768115942029,
      "grad_norm": 3.4380955696105957,
      "learning_rate": 3.762077294685991e-05,
      "loss": 1.9536,
      "step": 1025
    },
    {
      "epoch": 3.7318840579710146,
      "grad_norm": 3.822956085205078,
      "learning_rate": 3.756038647342995e-05,
      "loss": 1.8571,
      "step": 1030
    },
    {
      "epoch": 3.75,
      "grad_norm": 3.4271202087402344,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.9638,
      "step": 1035
    },
    {
      "epoch": 3.7681159420289854,
      "grad_norm": 3.7078607082366943,
      "learning_rate": 3.743961352657005e-05,
      "loss": 1.9524,
      "step": 1040
    },
    {
      "epoch": 3.786231884057971,
      "grad_norm": 2.4800333976745605,
      "learning_rate": 3.73792270531401e-05,
      "loss": 1.9101,
      "step": 1045
    },
    {
      "epoch": 3.8043478260869565,
      "grad_norm": 2.933522939682007,
      "learning_rate": 3.7318840579710144e-05,
      "loss": 2.0283,
      "step": 1050
    },
    {
      "epoch": 3.822463768115942,
      "grad_norm": 2.9955315589904785,
      "learning_rate": 3.7258454106280195e-05,
      "loss": 1.9143,
      "step": 1055
    },
    {
      "epoch": 3.8405797101449277,
      "grad_norm": 2.574080467224121,
      "learning_rate": 3.719806763285024e-05,
      "loss": 1.8988,
      "step": 1060
    },
    {
      "epoch": 3.858695652173913,
      "grad_norm": 3.4041411876678467,
      "learning_rate": 3.713768115942029e-05,
      "loss": 1.9416,
      "step": 1065
    },
    {
      "epoch": 3.8768115942028984,
      "grad_norm": 2.9809491634368896,
      "learning_rate": 3.7077294685990336e-05,
      "loss": 1.9165,
      "step": 1070
    },
    {
      "epoch": 3.894927536231884,
      "grad_norm": 3.6516902446746826,
      "learning_rate": 3.701690821256039e-05,
      "loss": 1.6501,
      "step": 1075
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 3.3813867568969727,
      "learning_rate": 3.695652173913043e-05,
      "loss": 2.0115,
      "step": 1080
    },
    {
      "epoch": 3.931159420289855,
      "grad_norm": 3.756943941116333,
      "learning_rate": 3.689613526570048e-05,
      "loss": 1.9586,
      "step": 1085
    },
    {
      "epoch": 3.949275362318841,
      "grad_norm": 3.5782744884490967,
      "learning_rate": 3.6835748792270534e-05,
      "loss": 1.9479,
      "step": 1090
    },
    {
      "epoch": 3.967391304347826,
      "grad_norm": 3.1335861682891846,
      "learning_rate": 3.6775362318840586e-05,
      "loss": 1.9338,
      "step": 1095
    },
    {
      "epoch": 3.9855072463768115,
      "grad_norm": 3.823634386062622,
      "learning_rate": 3.671497584541063e-05,
      "loss": 1.9552,
      "step": 1100
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.8981831073760986,
      "eval_runtime": 2.5962,
      "eval_samples_per_second": 18.873,
      "eval_steps_per_second": 2.696,
      "step": 1104
    },
    {
      "epoch": 4.003623188405797,
      "grad_norm": 3.269350051879883,
      "learning_rate": 3.665458937198068e-05,
      "loss": 1.9087,
      "step": 1105
    },
    {
      "epoch": 4.021739130434782,
      "grad_norm": 3.039698600769043,
      "learning_rate": 3.6594202898550726e-05,
      "loss": 1.8878,
      "step": 1110
    },
    {
      "epoch": 4.0398550724637685,
      "grad_norm": 3.076793909072876,
      "learning_rate": 3.653381642512078e-05,
      "loss": 1.8902,
      "step": 1115
    },
    {
      "epoch": 4.057971014492754,
      "grad_norm": 4.100532531738281,
      "learning_rate": 3.647342995169082e-05,
      "loss": 1.9659,
      "step": 1120
    },
    {
      "epoch": 4.076086956521739,
      "grad_norm": 3.3635852336883545,
      "learning_rate": 3.641304347826087e-05,
      "loss": 1.9065,
      "step": 1125
    },
    {
      "epoch": 4.094202898550725,
      "grad_norm": 2.934297561645508,
      "learning_rate": 3.635265700483092e-05,
      "loss": 1.879,
      "step": 1130
    },
    {
      "epoch": 4.11231884057971,
      "grad_norm": 3.1699318885803223,
      "learning_rate": 3.629227053140097e-05,
      "loss": 1.8933,
      "step": 1135
    },
    {
      "epoch": 4.130434782608695,
      "grad_norm": 3.3993968963623047,
      "learning_rate": 3.6231884057971014e-05,
      "loss": 1.8935,
      "step": 1140
    },
    {
      "epoch": 4.148550724637682,
      "grad_norm": 4.309797763824463,
      "learning_rate": 3.6171497584541065e-05,
      "loss": 1.9732,
      "step": 1145
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 3.1545450687408447,
      "learning_rate": 3.611111111111111e-05,
      "loss": 1.9476,
      "step": 1150
    },
    {
      "epoch": 4.184782608695652,
      "grad_norm": 3.025942802429199,
      "learning_rate": 3.605072463768116e-05,
      "loss": 1.9697,
      "step": 1155
    },
    {
      "epoch": 4.202898550724638,
      "grad_norm": 3.0329596996307373,
      "learning_rate": 3.5990338164251205e-05,
      "loss": 1.8784,
      "step": 1160
    },
    {
      "epoch": 4.221014492753623,
      "grad_norm": 3.4304661750793457,
      "learning_rate": 3.592995169082126e-05,
      "loss": 1.9214,
      "step": 1165
    },
    {
      "epoch": 4.239130434782608,
      "grad_norm": 2.813472270965576,
      "learning_rate": 3.58695652173913e-05,
      "loss": 1.9285,
      "step": 1170
    },
    {
      "epoch": 4.257246376811594,
      "grad_norm": 3.4205589294433594,
      "learning_rate": 3.580917874396135e-05,
      "loss": 1.8824,
      "step": 1175
    },
    {
      "epoch": 4.27536231884058,
      "grad_norm": 3.515324354171753,
      "learning_rate": 3.57487922705314e-05,
      "loss": 1.8815,
      "step": 1180
    },
    {
      "epoch": 4.293478260869565,
      "grad_norm": 2.3943729400634766,
      "learning_rate": 3.568840579710145e-05,
      "loss": 1.9159,
      "step": 1185
    },
    {
      "epoch": 4.311594202898551,
      "grad_norm": 3.355506658554077,
      "learning_rate": 3.56280193236715e-05,
      "loss": 1.8826,
      "step": 1190
    },
    {
      "epoch": 4.329710144927536,
      "grad_norm": 3.7188732624053955,
      "learning_rate": 3.556763285024155e-05,
      "loss": 1.9575,
      "step": 1195
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 3.4197070598602295,
      "learning_rate": 3.5507246376811596e-05,
      "loss": 1.9135,
      "step": 1200
    },
    {
      "epoch": 4.365942028985507,
      "grad_norm": 3.0761969089508057,
      "learning_rate": 3.544685990338165e-05,
      "loss": 1.8864,
      "step": 1205
    },
    {
      "epoch": 4.384057971014493,
      "grad_norm": 2.938262701034546,
      "learning_rate": 3.538647342995169e-05,
      "loss": 1.9428,
      "step": 1210
    },
    {
      "epoch": 4.4021739130434785,
      "grad_norm": 1.2350355386734009,
      "learning_rate": 3.532608695652174e-05,
      "loss": 1.6075,
      "step": 1215
    },
    {
      "epoch": 4.420289855072464,
      "grad_norm": 3.4008586406707764,
      "learning_rate": 3.526570048309179e-05,
      "loss": 1.9397,
      "step": 1220
    },
    {
      "epoch": 4.438405797101449,
      "grad_norm": 3.2752864360809326,
      "learning_rate": 3.520531400966184e-05,
      "loss": 1.8866,
      "step": 1225
    },
    {
      "epoch": 4.456521739130435,
      "grad_norm": 2.8102543354034424,
      "learning_rate": 3.514492753623188e-05,
      "loss": 1.9048,
      "step": 1230
    },
    {
      "epoch": 4.47463768115942,
      "grad_norm": 3.308058261871338,
      "learning_rate": 3.5084541062801935e-05,
      "loss": 1.93,
      "step": 1235
    },
    {
      "epoch": 4.492753623188406,
      "grad_norm": 3.0207929611206055,
      "learning_rate": 3.502415458937198e-05,
      "loss": 1.8438,
      "step": 1240
    },
    {
      "epoch": 4.510869565217392,
      "grad_norm": 2.7996206283569336,
      "learning_rate": 3.496376811594203e-05,
      "loss": 1.9534,
      "step": 1245
    },
    {
      "epoch": 4.528985507246377,
      "grad_norm": 3.910388946533203,
      "learning_rate": 3.490338164251208e-05,
      "loss": 1.9639,
      "step": 1250
    },
    {
      "epoch": 4.547101449275362,
      "grad_norm": 2.964984178543091,
      "learning_rate": 3.4842995169082126e-05,
      "loss": 1.9791,
      "step": 1255
    },
    {
      "epoch": 4.565217391304348,
      "grad_norm": 2.8049840927124023,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.943,
      "step": 1260
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 2.7854807376861572,
      "learning_rate": 3.472222222222222e-05,
      "loss": 1.9549,
      "step": 1265
    },
    {
      "epoch": 4.601449275362318,
      "grad_norm": 3.6677427291870117,
      "learning_rate": 3.4661835748792274e-05,
      "loss": 1.9859,
      "step": 1270
    },
    {
      "epoch": 4.619565217391305,
      "grad_norm": 3.3057148456573486,
      "learning_rate": 3.460144927536232e-05,
      "loss": 1.879,
      "step": 1275
    },
    {
      "epoch": 4.63768115942029,
      "grad_norm": 3.108093500137329,
      "learning_rate": 3.454106280193237e-05,
      "loss": 1.9364,
      "step": 1280
    },
    {
      "epoch": 4.655797101449275,
      "grad_norm": 2.5802500247955322,
      "learning_rate": 3.4480676328502414e-05,
      "loss": 1.9509,
      "step": 1285
    },
    {
      "epoch": 4.673913043478261,
      "grad_norm": 2.8580923080444336,
      "learning_rate": 3.4420289855072465e-05,
      "loss": 1.8565,
      "step": 1290
    },
    {
      "epoch": 4.692028985507246,
      "grad_norm": 2.4882800579071045,
      "learning_rate": 3.4359903381642517e-05,
      "loss": 1.8579,
      "step": 1295
    },
    {
      "epoch": 4.710144927536232,
      "grad_norm": 2.7946932315826416,
      "learning_rate": 3.429951690821256e-05,
      "loss": 1.9301,
      "step": 1300
    },
    {
      "epoch": 4.728260869565218,
      "grad_norm": 3.569939613342285,
      "learning_rate": 3.423913043478261e-05,
      "loss": 1.9595,
      "step": 1305
    },
    {
      "epoch": 4.746376811594203,
      "grad_norm": 2.966930389404297,
      "learning_rate": 3.4178743961352664e-05,
      "loss": 1.8695,
      "step": 1310
    },
    {
      "epoch": 4.7644927536231885,
      "grad_norm": 2.837601900100708,
      "learning_rate": 3.411835748792271e-05,
      "loss": 1.951,
      "step": 1315
    },
    {
      "epoch": 4.782608695652174,
      "grad_norm": 2.8064005374908447,
      "learning_rate": 3.405797101449276e-05,
      "loss": 1.9306,
      "step": 1320
    },
    {
      "epoch": 4.800724637681159,
      "grad_norm": 3.234114408493042,
      "learning_rate": 3.3997584541062804e-05,
      "loss": 1.9632,
      "step": 1325
    },
    {
      "epoch": 4.818840579710145,
      "grad_norm": 3.6980249881744385,
      "learning_rate": 3.3937198067632856e-05,
      "loss": 1.866,
      "step": 1330
    },
    {
      "epoch": 4.836956521739131,
      "grad_norm": 3.1725358963012695,
      "learning_rate": 3.38768115942029e-05,
      "loss": 1.9256,
      "step": 1335
    },
    {
      "epoch": 4.855072463768116,
      "grad_norm": 3.4356613159179688,
      "learning_rate": 3.381642512077295e-05,
      "loss": 1.8769,
      "step": 1340
    },
    {
      "epoch": 4.8731884057971016,
      "grad_norm": 3.432602643966675,
      "learning_rate": 3.3756038647342996e-05,
      "loss": 1.8891,
      "step": 1345
    },
    {
      "epoch": 4.891304347826087,
      "grad_norm": 2.657844305038452,
      "learning_rate": 3.369565217391305e-05,
      "loss": 1.9907,
      "step": 1350
    },
    {
      "epoch": 4.909420289855072,
      "grad_norm": 3.9045000076293945,
      "learning_rate": 3.363526570048309e-05,
      "loss": 1.9609,
      "step": 1355
    },
    {
      "epoch": 4.927536231884058,
      "grad_norm": 3.9977517127990723,
      "learning_rate": 3.357487922705314e-05,
      "loss": 1.8812,
      "step": 1360
    },
    {
      "epoch": 4.945652173913043,
      "grad_norm": 3.4206602573394775,
      "learning_rate": 3.351449275362319e-05,
      "loss": 1.8752,
      "step": 1365
    },
    {
      "epoch": 4.963768115942029,
      "grad_norm": 2.922563314437866,
      "learning_rate": 3.345410628019324e-05,
      "loss": 1.9332,
      "step": 1370
    },
    {
      "epoch": 4.981884057971015,
      "grad_norm": 3.132472276687622,
      "learning_rate": 3.3393719806763284e-05,
      "loss": 1.9304,
      "step": 1375
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.1222548484802246,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.9186,
      "step": 1380
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8726322650909424,
      "eval_runtime": 2.532,
      "eval_samples_per_second": 19.352,
      "eval_steps_per_second": 2.765,
      "step": 1380
    },
    {
      "epoch": 5.018115942028985,
      "grad_norm": 4.743859767913818,
      "learning_rate": 3.327294685990338e-05,
      "loss": 1.9054,
      "step": 1385
    },
    {
      "epoch": 5.036231884057971,
      "grad_norm": 3.6249001026153564,
      "learning_rate": 3.321256038647343e-05,
      "loss": 1.8059,
      "step": 1390
    },
    {
      "epoch": 5.054347826086956,
      "grad_norm": 3.5929195880889893,
      "learning_rate": 3.3152173913043475e-05,
      "loss": 1.8782,
      "step": 1395
    },
    {
      "epoch": 5.072463768115942,
      "grad_norm": 2.9506537914276123,
      "learning_rate": 3.3091787439613533e-05,
      "loss": 1.8943,
      "step": 1400
    },
    {
      "epoch": 5.090579710144928,
      "grad_norm": 3.2492928504943848,
      "learning_rate": 3.303140096618358e-05,
      "loss": 1.8292,
      "step": 1405
    },
    {
      "epoch": 5.108695652173913,
      "grad_norm": 3.8106775283813477,
      "learning_rate": 3.297101449275363e-05,
      "loss": 1.9073,
      "step": 1410
    },
    {
      "epoch": 5.1268115942028984,
      "grad_norm": 3.4401586055755615,
      "learning_rate": 3.2910628019323674e-05,
      "loss": 1.8591,
      "step": 1415
    },
    {
      "epoch": 5.144927536231884,
      "grad_norm": 3.2431108951568604,
      "learning_rate": 3.2850241545893725e-05,
      "loss": 1.9075,
      "step": 1420
    },
    {
      "epoch": 5.163043478260869,
      "grad_norm": 3.865323066711426,
      "learning_rate": 3.278985507246377e-05,
      "loss": 1.8061,
      "step": 1425
    },
    {
      "epoch": 5.181159420289855,
      "grad_norm": 3.127107858657837,
      "learning_rate": 3.272946859903382e-05,
      "loss": 1.8908,
      "step": 1430
    },
    {
      "epoch": 5.199275362318841,
      "grad_norm": 2.9339182376861572,
      "learning_rate": 3.2669082125603866e-05,
      "loss": 1.9267,
      "step": 1435
    },
    {
      "epoch": 5.217391304347826,
      "grad_norm": 2.658393621444702,
      "learning_rate": 3.260869565217392e-05,
      "loss": 1.9195,
      "step": 1440
    },
    {
      "epoch": 5.2355072463768115,
      "grad_norm": 2.9248909950256348,
      "learning_rate": 3.254830917874396e-05,
      "loss": 1.9305,
      "step": 1445
    },
    {
      "epoch": 5.253623188405797,
      "grad_norm": 2.978713274002075,
      "learning_rate": 3.248792270531401e-05,
      "loss": 1.9585,
      "step": 1450
    },
    {
      "epoch": 5.271739130434782,
      "grad_norm": 2.608670711517334,
      "learning_rate": 3.242753623188406e-05,
      "loss": 1.8977,
      "step": 1455
    },
    {
      "epoch": 5.2898550724637685,
      "grad_norm": 3.0297601222991943,
      "learning_rate": 3.236714975845411e-05,
      "loss": 1.8843,
      "step": 1460
    },
    {
      "epoch": 5.307971014492754,
      "grad_norm": 3.165163516998291,
      "learning_rate": 3.230676328502415e-05,
      "loss": 1.9494,
      "step": 1465
    },
    {
      "epoch": 5.326086956521739,
      "grad_norm": 2.7618491649627686,
      "learning_rate": 3.2246376811594205e-05,
      "loss": 1.9429,
      "step": 1470
    },
    {
      "epoch": 5.344202898550725,
      "grad_norm": 3.34942889213562,
      "learning_rate": 3.218599033816425e-05,
      "loss": 1.9198,
      "step": 1475
    },
    {
      "epoch": 5.36231884057971,
      "grad_norm": 2.6124587059020996,
      "learning_rate": 3.21256038647343e-05,
      "loss": 1.8683,
      "step": 1480
    },
    {
      "epoch": 5.380434782608695,
      "grad_norm": 3.0111329555511475,
      "learning_rate": 3.2065217391304345e-05,
      "loss": 1.9697,
      "step": 1485
    },
    {
      "epoch": 5.398550724637682,
      "grad_norm": 2.9280834197998047,
      "learning_rate": 3.2004830917874396e-05,
      "loss": 1.9331,
      "step": 1490
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 2.8427622318267822,
      "learning_rate": 3.194444444444444e-05,
      "loss": 1.9196,
      "step": 1495
    },
    {
      "epoch": 5.434782608695652,
      "grad_norm": 3.017479419708252,
      "learning_rate": 3.188405797101449e-05,
      "loss": 1.8961,
      "step": 1500
    },
    {
      "epoch": 5.452898550724638,
      "grad_norm": 2.7271530628204346,
      "learning_rate": 3.1823671497584544e-05,
      "loss": 1.8506,
      "step": 1505
    },
    {
      "epoch": 5.471014492753623,
      "grad_norm": 3.366628885269165,
      "learning_rate": 3.1763285024154595e-05,
      "loss": 1.9185,
      "step": 1510
    },
    {
      "epoch": 5.489130434782608,
      "grad_norm": 3.2973721027374268,
      "learning_rate": 3.170289855072464e-05,
      "loss": 1.8666,
      "step": 1515
    },
    {
      "epoch": 5.507246376811594,
      "grad_norm": 2.881004810333252,
      "learning_rate": 3.164251207729469e-05,
      "loss": 1.8862,
      "step": 1520
    },
    {
      "epoch": 5.52536231884058,
      "grad_norm": 3.1666548252105713,
      "learning_rate": 3.1582125603864735e-05,
      "loss": 1.848,
      "step": 1525
    },
    {
      "epoch": 5.543478260869565,
      "grad_norm": 2.985020399093628,
      "learning_rate": 3.152173913043479e-05,
      "loss": 1.9545,
      "step": 1530
    },
    {
      "epoch": 5.561594202898551,
      "grad_norm": 3.408038377761841,
      "learning_rate": 3.146135265700483e-05,
      "loss": 1.9032,
      "step": 1535
    },
    {
      "epoch": 5.579710144927536,
      "grad_norm": 4.286205291748047,
      "learning_rate": 3.140096618357488e-05,
      "loss": 1.8225,
      "step": 1540
    },
    {
      "epoch": 5.5978260869565215,
      "grad_norm": 3.125626802444458,
      "learning_rate": 3.134057971014493e-05,
      "loss": 1.8751,
      "step": 1545
    },
    {
      "epoch": 5.615942028985507,
      "grad_norm": 3.3608689308166504,
      "learning_rate": 3.128019323671498e-05,
      "loss": 1.8729,
      "step": 1550
    },
    {
      "epoch": 5.634057971014493,
      "grad_norm": 3.4577503204345703,
      "learning_rate": 3.121980676328502e-05,
      "loss": 1.7958,
      "step": 1555
    },
    {
      "epoch": 5.6521739130434785,
      "grad_norm": 3.7328758239746094,
      "learning_rate": 3.1159420289855074e-05,
      "loss": 1.9163,
      "step": 1560
    },
    {
      "epoch": 5.670289855072464,
      "grad_norm": 3.314202308654785,
      "learning_rate": 3.109903381642512e-05,
      "loss": 1.8895,
      "step": 1565
    },
    {
      "epoch": 5.688405797101449,
      "grad_norm": 3.2043538093566895,
      "learning_rate": 3.103864734299517e-05,
      "loss": 1.8626,
      "step": 1570
    },
    {
      "epoch": 5.706521739130435,
      "grad_norm": 2.985086441040039,
      "learning_rate": 3.0978260869565215e-05,
      "loss": 1.869,
      "step": 1575
    },
    {
      "epoch": 5.72463768115942,
      "grad_norm": 3.0697109699249268,
      "learning_rate": 3.0917874396135266e-05,
      "loss": 1.8531,
      "step": 1580
    },
    {
      "epoch": 5.742753623188406,
      "grad_norm": 2.8245785236358643,
      "learning_rate": 3.085748792270531e-05,
      "loss": 1.8582,
      "step": 1585
    },
    {
      "epoch": 5.760869565217392,
      "grad_norm": 2.9103052616119385,
      "learning_rate": 3.079710144927536e-05,
      "loss": 1.5909,
      "step": 1590
    },
    {
      "epoch": 5.778985507246377,
      "grad_norm": 4.024622917175293,
      "learning_rate": 3.073671497584541e-05,
      "loss": 1.9099,
      "step": 1595
    },
    {
      "epoch": 5.797101449275362,
      "grad_norm": 2.778642177581787,
      "learning_rate": 3.067632850241546e-05,
      "loss": 1.9624,
      "step": 1600
    },
    {
      "epoch": 5.815217391304348,
      "grad_norm": 3.361154317855835,
      "learning_rate": 3.061594202898551e-05,
      "loss": 1.837,
      "step": 1605
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 2.716398239135742,
      "learning_rate": 3.055555555555556e-05,
      "loss": 1.8714,
      "step": 1610
    },
    {
      "epoch": 5.851449275362318,
      "grad_norm": 4.399161338806152,
      "learning_rate": 3.049516908212561e-05,
      "loss": 1.85,
      "step": 1615
    },
    {
      "epoch": 5.869565217391305,
      "grad_norm": 3.124363899230957,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 1.8572,
      "step": 1620
    },
    {
      "epoch": 5.88768115942029,
      "grad_norm": 3.6931581497192383,
      "learning_rate": 3.0374396135265704e-05,
      "loss": 1.8633,
      "step": 1625
    },
    {
      "epoch": 5.905797101449275,
      "grad_norm": 2.7152907848358154,
      "learning_rate": 3.0314009661835752e-05,
      "loss": 1.8788,
      "step": 1630
    },
    {
      "epoch": 5.923913043478261,
      "grad_norm": 2.9581661224365234,
      "learning_rate": 3.02536231884058e-05,
      "loss": 1.8931,
      "step": 1635
    },
    {
      "epoch": 5.942028985507246,
      "grad_norm": 3.527423620223999,
      "learning_rate": 3.0193236714975848e-05,
      "loss": 1.933,
      "step": 1640
    },
    {
      "epoch": 5.960144927536232,
      "grad_norm": 4.1773271560668945,
      "learning_rate": 3.0132850241545896e-05,
      "loss": 1.9258,
      "step": 1645
    },
    {
      "epoch": 5.978260869565218,
      "grad_norm": 3.4519877433776855,
      "learning_rate": 3.0072463768115944e-05,
      "loss": 1.9364,
      "step": 1650
    },
    {
      "epoch": 5.996376811594203,
      "grad_norm": 3.3607566356658936,
      "learning_rate": 3.0012077294685992e-05,
      "loss": 1.87,
      "step": 1655
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8604114055633545,
      "eval_runtime": 2.4634,
      "eval_samples_per_second": 19.891,
      "eval_steps_per_second": 2.842,
      "step": 1656
    },
    {
      "epoch": 6.0144927536231885,
      "grad_norm": 3.240701913833618,
      "learning_rate": 2.995169082125604e-05,
      "loss": 1.9318,
      "step": 1660
    },
    {
      "epoch": 6.032608695652174,
      "grad_norm": 2.787102460861206,
      "learning_rate": 2.9891304347826088e-05,
      "loss": 1.8381,
      "step": 1665
    },
    {
      "epoch": 6.050724637681159,
      "grad_norm": 2.7109575271606445,
      "learning_rate": 2.9830917874396136e-05,
      "loss": 1.8793,
      "step": 1670
    },
    {
      "epoch": 6.068840579710145,
      "grad_norm": 2.820847272872925,
      "learning_rate": 2.9770531400966184e-05,
      "loss": 1.8777,
      "step": 1675
    },
    {
      "epoch": 6.086956521739131,
      "grad_norm": 3.904817819595337,
      "learning_rate": 2.971014492753623e-05,
      "loss": 1.8297,
      "step": 1680
    },
    {
      "epoch": 6.105072463768116,
      "grad_norm": 2.8170931339263916,
      "learning_rate": 2.964975845410628e-05,
      "loss": 1.803,
      "step": 1685
    },
    {
      "epoch": 6.1231884057971016,
      "grad_norm": 3.3054871559143066,
      "learning_rate": 2.9589371980676327e-05,
      "loss": 1.8281,
      "step": 1690
    },
    {
      "epoch": 6.141304347826087,
      "grad_norm": 3.3338327407836914,
      "learning_rate": 2.9528985507246375e-05,
      "loss": 1.8893,
      "step": 1695
    },
    {
      "epoch": 6.159420289855072,
      "grad_norm": 4.003791332244873,
      "learning_rate": 2.9468599033816423e-05,
      "loss": 1.8139,
      "step": 1700
    },
    {
      "epoch": 6.177536231884058,
      "grad_norm": 2.5991103649139404,
      "learning_rate": 2.940821256038647e-05,
      "loss": 1.8966,
      "step": 1705
    },
    {
      "epoch": 6.195652173913044,
      "grad_norm": 2.7679975032806396,
      "learning_rate": 2.9347826086956526e-05,
      "loss": 1.8382,
      "step": 1710
    },
    {
      "epoch": 6.213768115942029,
      "grad_norm": 3.582531690597534,
      "learning_rate": 2.9287439613526574e-05,
      "loss": 1.8137,
      "step": 1715
    },
    {
      "epoch": 6.231884057971015,
      "grad_norm": 3.4135632514953613,
      "learning_rate": 2.9227053140096622e-05,
      "loss": 1.9038,
      "step": 1720
    },
    {
      "epoch": 6.25,
      "grad_norm": 3.3594210147857666,
      "learning_rate": 2.916666666666667e-05,
      "loss": 1.8763,
      "step": 1725
    },
    {
      "epoch": 6.268115942028985,
      "grad_norm": 3.1400387287139893,
      "learning_rate": 2.9106280193236718e-05,
      "loss": 1.811,
      "step": 1730
    },
    {
      "epoch": 6.286231884057971,
      "grad_norm": 3.1897177696228027,
      "learning_rate": 2.9045893719806766e-05,
      "loss": 1.8626,
      "step": 1735
    },
    {
      "epoch": 6.304347826086957,
      "grad_norm": 2.934699535369873,
      "learning_rate": 2.8985507246376814e-05,
      "loss": 1.8029,
      "step": 1740
    },
    {
      "epoch": 6.322463768115942,
      "grad_norm": 4.150903224945068,
      "learning_rate": 2.892512077294686e-05,
      "loss": 1.9368,
      "step": 1745
    },
    {
      "epoch": 6.340579710144928,
      "grad_norm": 3.7371768951416016,
      "learning_rate": 2.886473429951691e-05,
      "loss": 1.9607,
      "step": 1750
    },
    {
      "epoch": 6.358695652173913,
      "grad_norm": 3.531270742416382,
      "learning_rate": 2.8804347826086957e-05,
      "loss": 1.8614,
      "step": 1755
    },
    {
      "epoch": 6.3768115942028984,
      "grad_norm": 2.798560857772827,
      "learning_rate": 2.8743961352657005e-05,
      "loss": 1.872,
      "step": 1760
    },
    {
      "epoch": 6.394927536231884,
      "grad_norm": 3.3081140518188477,
      "learning_rate": 2.8683574879227053e-05,
      "loss": 1.9106,
      "step": 1765
    },
    {
      "epoch": 6.413043478260869,
      "grad_norm": 2.9882309436798096,
      "learning_rate": 2.86231884057971e-05,
      "loss": 1.8946,
      "step": 1770
    },
    {
      "epoch": 6.431159420289855,
      "grad_norm": 3.2620935440063477,
      "learning_rate": 2.856280193236715e-05,
      "loss": 1.9262,
      "step": 1775
    },
    {
      "epoch": 6.449275362318841,
      "grad_norm": 3.552741050720215,
      "learning_rate": 2.8502415458937197e-05,
      "loss": 1.7995,
      "step": 1780
    },
    {
      "epoch": 6.467391304347826,
      "grad_norm": 2.683220863342285,
      "learning_rate": 2.8442028985507245e-05,
      "loss": 1.916,
      "step": 1785
    },
    {
      "epoch": 6.4855072463768115,
      "grad_norm": 2.988461494445801,
      "learning_rate": 2.8381642512077293e-05,
      "loss": 1.8876,
      "step": 1790
    },
    {
      "epoch": 6.503623188405797,
      "grad_norm": 2.6926944255828857,
      "learning_rate": 2.832125603864734e-05,
      "loss": 1.9096,
      "step": 1795
    },
    {
      "epoch": 6.521739130434782,
      "grad_norm": 3.5437777042388916,
      "learning_rate": 2.826086956521739e-05,
      "loss": 1.8087,
      "step": 1800
    },
    {
      "epoch": 6.539855072463768,
      "grad_norm": 2.953399181365967,
      "learning_rate": 2.820048309178744e-05,
      "loss": 1.8794,
      "step": 1805
    },
    {
      "epoch": 6.557971014492754,
      "grad_norm": 2.9754860401153564,
      "learning_rate": 2.8140096618357488e-05,
      "loss": 1.8751,
      "step": 1810
    },
    {
      "epoch": 6.576086956521739,
      "grad_norm": 3.3953001499176025,
      "learning_rate": 2.807971014492754e-05,
      "loss": 1.8613,
      "step": 1815
    },
    {
      "epoch": 6.594202898550725,
      "grad_norm": 2.8027596473693848,
      "learning_rate": 2.8019323671497587e-05,
      "loss": 1.8364,
      "step": 1820
    },
    {
      "epoch": 6.61231884057971,
      "grad_norm": 3.049888849258423,
      "learning_rate": 2.7958937198067635e-05,
      "loss": 1.8426,
      "step": 1825
    },
    {
      "epoch": 6.630434782608695,
      "grad_norm": 3.4894626140594482,
      "learning_rate": 2.7898550724637683e-05,
      "loss": 1.899,
      "step": 1830
    },
    {
      "epoch": 6.648550724637682,
      "grad_norm": 2.7882251739501953,
      "learning_rate": 2.783816425120773e-05,
      "loss": 1.9286,
      "step": 1835
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 3.4377036094665527,
      "learning_rate": 2.777777777777778e-05,
      "loss": 1.9003,
      "step": 1840
    },
    {
      "epoch": 6.684782608695652,
      "grad_norm": 3.935828447341919,
      "learning_rate": 2.7717391304347827e-05,
      "loss": 1.8691,
      "step": 1845
    },
    {
      "epoch": 6.702898550724638,
      "grad_norm": 3.4857680797576904,
      "learning_rate": 2.7657004830917875e-05,
      "loss": 1.8298,
      "step": 1850
    },
    {
      "epoch": 6.721014492753623,
      "grad_norm": 2.5934884548187256,
      "learning_rate": 2.7596618357487923e-05,
      "loss": 1.8249,
      "step": 1855
    },
    {
      "epoch": 6.739130434782608,
      "grad_norm": 2.742833137512207,
      "learning_rate": 2.753623188405797e-05,
      "loss": 1.9193,
      "step": 1860
    },
    {
      "epoch": 6.757246376811594,
      "grad_norm": 2.878805637359619,
      "learning_rate": 2.7475845410628022e-05,
      "loss": 1.9117,
      "step": 1865
    },
    {
      "epoch": 6.77536231884058,
      "grad_norm": 3.591371536254883,
      "learning_rate": 2.741545893719807e-05,
      "loss": 1.826,
      "step": 1870
    },
    {
      "epoch": 6.793478260869565,
      "grad_norm": 3.2799737453460693,
      "learning_rate": 2.7355072463768118e-05,
      "loss": 1.8395,
      "step": 1875
    },
    {
      "epoch": 6.811594202898551,
      "grad_norm": 3.115300416946411,
      "learning_rate": 2.7294685990338166e-05,
      "loss": 1.8552,
      "step": 1880
    },
    {
      "epoch": 6.829710144927536,
      "grad_norm": 3.0979182720184326,
      "learning_rate": 2.7234299516908214e-05,
      "loss": 1.8373,
      "step": 1885
    },
    {
      "epoch": 6.8478260869565215,
      "grad_norm": 3.5950934886932373,
      "learning_rate": 2.7173913043478262e-05,
      "loss": 1.8952,
      "step": 1890
    },
    {
      "epoch": 6.865942028985507,
      "grad_norm": 2.5434656143188477,
      "learning_rate": 2.711352657004831e-05,
      "loss": 1.7848,
      "step": 1895
    },
    {
      "epoch": 6.884057971014493,
      "grad_norm": 3.4033713340759277,
      "learning_rate": 2.7053140096618358e-05,
      "loss": 1.8593,
      "step": 1900
    },
    {
      "epoch": 6.9021739130434785,
      "grad_norm": 2.773145914077759,
      "learning_rate": 2.6992753623188406e-05,
      "loss": 1.5876,
      "step": 1905
    },
    {
      "epoch": 6.920289855072464,
      "grad_norm": 2.6842639446258545,
      "learning_rate": 2.6932367149758454e-05,
      "loss": 1.8641,
      "step": 1910
    },
    {
      "epoch": 6.938405797101449,
      "grad_norm": 2.9586827754974365,
      "learning_rate": 2.6871980676328505e-05,
      "loss": 1.8553,
      "step": 1915
    },
    {
      "epoch": 6.956521739130435,
      "grad_norm": 3.1558525562286377,
      "learning_rate": 2.6811594202898553e-05,
      "loss": 1.8615,
      "step": 1920
    },
    {
      "epoch": 6.97463768115942,
      "grad_norm": 4.004144191741943,
      "learning_rate": 2.6751207729468604e-05,
      "loss": 1.8969,
      "step": 1925
    },
    {
      "epoch": 6.992753623188406,
      "grad_norm": 3.189626932144165,
      "learning_rate": 2.6690821256038652e-05,
      "loss": 1.8138,
      "step": 1930
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8546637296676636,
      "eval_runtime": 2.4769,
      "eval_samples_per_second": 19.783,
      "eval_steps_per_second": 2.826,
      "step": 1932
    },
    {
      "epoch": 7.010869565217392,
      "grad_norm": 2.837423086166382,
      "learning_rate": 2.66304347826087e-05,
      "loss": 1.8026,
      "step": 1935
    },
    {
      "epoch": 7.028985507246377,
      "grad_norm": 2.9778964519500732,
      "learning_rate": 2.6570048309178748e-05,
      "loss": 1.8684,
      "step": 1940
    },
    {
      "epoch": 7.047101449275362,
      "grad_norm": 3.2888269424438477,
      "learning_rate": 2.6509661835748796e-05,
      "loss": 1.7567,
      "step": 1945
    },
    {
      "epoch": 7.065217391304348,
      "grad_norm": 3.402496099472046,
      "learning_rate": 2.6449275362318844e-05,
      "loss": 1.8589,
      "step": 1950
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 3.4103281497955322,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 1.829,
      "step": 1955
    },
    {
      "epoch": 7.101449275362318,
      "grad_norm": 4.127458095550537,
      "learning_rate": 2.632850241545894e-05,
      "loss": 1.8191,
      "step": 1960
    },
    {
      "epoch": 7.119565217391305,
      "grad_norm": 3.4856348037719727,
      "learning_rate": 2.6268115942028988e-05,
      "loss": 1.8043,
      "step": 1965
    },
    {
      "epoch": 7.13768115942029,
      "grad_norm": 2.943934679031372,
      "learning_rate": 2.6207729468599036e-05,
      "loss": 1.7933,
      "step": 1970
    },
    {
      "epoch": 7.155797101449275,
      "grad_norm": 3.5065736770629883,
      "learning_rate": 2.6147342995169084e-05,
      "loss": 1.859,
      "step": 1975
    },
    {
      "epoch": 7.173913043478261,
      "grad_norm": 3.5974485874176025,
      "learning_rate": 2.608695652173913e-05,
      "loss": 1.8534,
      "step": 1980
    },
    {
      "epoch": 7.192028985507246,
      "grad_norm": 2.9145095348358154,
      "learning_rate": 2.602657004830918e-05,
      "loss": 1.8315,
      "step": 1985
    },
    {
      "epoch": 7.2101449275362315,
      "grad_norm": 3.5221385955810547,
      "learning_rate": 2.5966183574879227e-05,
      "loss": 1.8234,
      "step": 1990
    },
    {
      "epoch": 7.228260869565218,
      "grad_norm": 4.035319805145264,
      "learning_rate": 2.5905797101449275e-05,
      "loss": 1.8549,
      "step": 1995
    },
    {
      "epoch": 7.246376811594203,
      "grad_norm": 3.5055792331695557,
      "learning_rate": 2.5845410628019323e-05,
      "loss": 1.8473,
      "step": 2000
    },
    {
      "epoch": 7.2644927536231885,
      "grad_norm": 5.896585941314697,
      "learning_rate": 2.578502415458937e-05,
      "loss": 1.8114,
      "step": 2005
    },
    {
      "epoch": 7.282608695652174,
      "grad_norm": 3.601954936981201,
      "learning_rate": 2.572463768115942e-05,
      "loss": 1.8815,
      "step": 2010
    },
    {
      "epoch": 7.300724637681159,
      "grad_norm": 3.0495786666870117,
      "learning_rate": 2.5664251207729467e-05,
      "loss": 1.8267,
      "step": 2015
    },
    {
      "epoch": 7.318840579710145,
      "grad_norm": 3.239156723022461,
      "learning_rate": 2.5603864734299522e-05,
      "loss": 1.8651,
      "step": 2020
    },
    {
      "epoch": 7.336956521739131,
      "grad_norm": 4.631726264953613,
      "learning_rate": 2.554347826086957e-05,
      "loss": 1.8707,
      "step": 2025
    },
    {
      "epoch": 7.355072463768116,
      "grad_norm": 3.3649306297302246,
      "learning_rate": 2.5483091787439618e-05,
      "loss": 1.8379,
      "step": 2030
    },
    {
      "epoch": 7.3731884057971016,
      "grad_norm": 3.629829168319702,
      "learning_rate": 2.5422705314009666e-05,
      "loss": 1.8889,
      "step": 2035
    },
    {
      "epoch": 7.391304347826087,
      "grad_norm": 3.675966262817383,
      "learning_rate": 2.5362318840579714e-05,
      "loss": 1.8846,
      "step": 2040
    },
    {
      "epoch": 7.409420289855072,
      "grad_norm": 3.0346412658691406,
      "learning_rate": 2.530193236714976e-05,
      "loss": 1.8245,
      "step": 2045
    },
    {
      "epoch": 7.427536231884058,
      "grad_norm": 2.976774215698242,
      "learning_rate": 2.524154589371981e-05,
      "loss": 1.876,
      "step": 2050
    },
    {
      "epoch": 7.445652173913043,
      "grad_norm": 3.1425790786743164,
      "learning_rate": 2.5181159420289857e-05,
      "loss": 1.8293,
      "step": 2055
    },
    {
      "epoch": 7.463768115942029,
      "grad_norm": 3.162708044052124,
      "learning_rate": 2.5120772946859905e-05,
      "loss": 1.8261,
      "step": 2060
    },
    {
      "epoch": 7.481884057971015,
      "grad_norm": 3.0248184204101562,
      "learning_rate": 2.5060386473429953e-05,
      "loss": 1.8255,
      "step": 2065
    },
    {
      "epoch": 7.5,
      "grad_norm": 3.861917018890381,
      "learning_rate": 2.5e-05,
      "loss": 1.9141,
      "step": 2070
    },
    {
      "epoch": 7.518115942028985,
      "grad_norm": 2.7904813289642334,
      "learning_rate": 2.493961352657005e-05,
      "loss": 1.8061,
      "step": 2075
    },
    {
      "epoch": 7.536231884057971,
      "grad_norm": 2.7033989429473877,
      "learning_rate": 2.4879227053140097e-05,
      "loss": 1.8704,
      "step": 2080
    },
    {
      "epoch": 7.554347826086957,
      "grad_norm": 3.1160237789154053,
      "learning_rate": 2.4818840579710145e-05,
      "loss": 1.8579,
      "step": 2085
    },
    {
      "epoch": 7.572463768115942,
      "grad_norm": 2.968911647796631,
      "learning_rate": 2.4758454106280193e-05,
      "loss": 1.7983,
      "step": 2090
    },
    {
      "epoch": 7.590579710144928,
      "grad_norm": 2.8807222843170166,
      "learning_rate": 2.469806763285024e-05,
      "loss": 1.8042,
      "step": 2095
    },
    {
      "epoch": 7.608695652173913,
      "grad_norm": 3.5322277545928955,
      "learning_rate": 2.4637681159420292e-05,
      "loss": 1.8432,
      "step": 2100
    },
    {
      "epoch": 7.6268115942028984,
      "grad_norm": 2.6483094692230225,
      "learning_rate": 2.457729468599034e-05,
      "loss": 1.8537,
      "step": 2105
    },
    {
      "epoch": 7.644927536231884,
      "grad_norm": 2.917480707168579,
      "learning_rate": 2.4516908212560388e-05,
      "loss": 1.8928,
      "step": 2110
    },
    {
      "epoch": 7.663043478260869,
      "grad_norm": 3.8263356685638428,
      "learning_rate": 2.4456521739130436e-05,
      "loss": 1.8568,
      "step": 2115
    },
    {
      "epoch": 7.681159420289855,
      "grad_norm": 3.23820161819458,
      "learning_rate": 2.4396135265700484e-05,
      "loss": 1.8848,
      "step": 2120
    },
    {
      "epoch": 7.699275362318841,
      "grad_norm": 3.4006564617156982,
      "learning_rate": 2.4335748792270532e-05,
      "loss": 1.8358,
      "step": 2125
    },
    {
      "epoch": 7.717391304347826,
      "grad_norm": 2.9571447372436523,
      "learning_rate": 2.427536231884058e-05,
      "loss": 1.8613,
      "step": 2130
    },
    {
      "epoch": 7.7355072463768115,
      "grad_norm": 2.8353734016418457,
      "learning_rate": 2.4214975845410628e-05,
      "loss": 1.8452,
      "step": 2135
    },
    {
      "epoch": 7.753623188405797,
      "grad_norm": 3.9412477016448975,
      "learning_rate": 2.4154589371980676e-05,
      "loss": 1.8414,
      "step": 2140
    },
    {
      "epoch": 7.771739130434782,
      "grad_norm": 3.0407629013061523,
      "learning_rate": 2.4094202898550724e-05,
      "loss": 1.878,
      "step": 2145
    },
    {
      "epoch": 7.789855072463768,
      "grad_norm": 3.6326088905334473,
      "learning_rate": 2.4033816425120775e-05,
      "loss": 1.8583,
      "step": 2150
    },
    {
      "epoch": 7.807971014492754,
      "grad_norm": 3.8679280281066895,
      "learning_rate": 2.3973429951690823e-05,
      "loss": 1.8806,
      "step": 2155
    },
    {
      "epoch": 7.826086956521739,
      "grad_norm": 3.1299848556518555,
      "learning_rate": 2.391304347826087e-05,
      "loss": 1.878,
      "step": 2160
    },
    {
      "epoch": 7.844202898550725,
      "grad_norm": 2.9201161861419678,
      "learning_rate": 2.385265700483092e-05,
      "loss": 1.5147,
      "step": 2165
    },
    {
      "epoch": 7.86231884057971,
      "grad_norm": 3.2836456298828125,
      "learning_rate": 2.3792270531400967e-05,
      "loss": 1.855,
      "step": 2170
    },
    {
      "epoch": 7.880434782608695,
      "grad_norm": 3.440678119659424,
      "learning_rate": 2.3731884057971015e-05,
      "loss": 1.8762,
      "step": 2175
    },
    {
      "epoch": 7.898550724637682,
      "grad_norm": 2.763054847717285,
      "learning_rate": 2.3671497584541063e-05,
      "loss": 1.8848,
      "step": 2180
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 3.14334774017334,
      "learning_rate": 2.361111111111111e-05,
      "loss": 1.8095,
      "step": 2185
    },
    {
      "epoch": 7.934782608695652,
      "grad_norm": 2.890153169631958,
      "learning_rate": 2.355072463768116e-05,
      "loss": 1.8627,
      "step": 2190
    },
    {
      "epoch": 7.952898550724638,
      "grad_norm": 2.914571762084961,
      "learning_rate": 2.3490338164251206e-05,
      "loss": 1.8013,
      "step": 2195
    },
    {
      "epoch": 7.971014492753623,
      "grad_norm": 2.911855936050415,
      "learning_rate": 2.3429951690821258e-05,
      "loss": 1.8247,
      "step": 2200
    },
    {
      "epoch": 7.989130434782608,
      "grad_norm": 2.5175535678863525,
      "learning_rate": 2.3369565217391306e-05,
      "loss": 1.8276,
      "step": 2205
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8345699310302734,
      "eval_runtime": 2.5118,
      "eval_samples_per_second": 19.508,
      "eval_steps_per_second": 2.787,
      "step": 2208
    },
    {
      "epoch": 8.007246376811594,
      "grad_norm": 3.320162296295166,
      "learning_rate": 2.3309178743961354e-05,
      "loss": 1.8349,
      "step": 2210
    },
    {
      "epoch": 8.02536231884058,
      "grad_norm": 3.5715301036834717,
      "learning_rate": 2.32487922705314e-05,
      "loss": 1.7591,
      "step": 2215
    },
    {
      "epoch": 8.043478260869565,
      "grad_norm": 3.1429004669189453,
      "learning_rate": 2.318840579710145e-05,
      "loss": 1.783,
      "step": 2220
    },
    {
      "epoch": 8.06159420289855,
      "grad_norm": 3.23435115814209,
      "learning_rate": 2.3128019323671497e-05,
      "loss": 1.894,
      "step": 2225
    },
    {
      "epoch": 8.079710144927537,
      "grad_norm": 3.386871099472046,
      "learning_rate": 2.3067632850241545e-05,
      "loss": 1.8378,
      "step": 2230
    },
    {
      "epoch": 8.097826086956522,
      "grad_norm": 3.057417631149292,
      "learning_rate": 2.3007246376811593e-05,
      "loss": 1.8673,
      "step": 2235
    },
    {
      "epoch": 8.115942028985508,
      "grad_norm": 2.688206672668457,
      "learning_rate": 2.294685990338164e-05,
      "loss": 1.8372,
      "step": 2240
    },
    {
      "epoch": 8.134057971014492,
      "grad_norm": 4.15178108215332,
      "learning_rate": 2.288647342995169e-05,
      "loss": 1.8172,
      "step": 2245
    },
    {
      "epoch": 8.152173913043478,
      "grad_norm": 3.3286073207855225,
      "learning_rate": 2.282608695652174e-05,
      "loss": 1.7728,
      "step": 2250
    },
    {
      "epoch": 8.170289855072463,
      "grad_norm": 3.679487705230713,
      "learning_rate": 2.276570048309179e-05,
      "loss": 1.7746,
      "step": 2255
    },
    {
      "epoch": 8.18840579710145,
      "grad_norm": 3.1383674144744873,
      "learning_rate": 2.2705314009661836e-05,
      "loss": 1.8068,
      "step": 2260
    },
    {
      "epoch": 8.206521739130435,
      "grad_norm": 3.6393635272979736,
      "learning_rate": 2.2644927536231884e-05,
      "loss": 1.8427,
      "step": 2265
    },
    {
      "epoch": 8.22463768115942,
      "grad_norm": 3.1820218563079834,
      "learning_rate": 2.2584541062801932e-05,
      "loss": 1.8157,
      "step": 2270
    },
    {
      "epoch": 8.242753623188406,
      "grad_norm": 2.6955513954162598,
      "learning_rate": 2.252415458937198e-05,
      "loss": 1.8079,
      "step": 2275
    },
    {
      "epoch": 8.26086956521739,
      "grad_norm": 2.6987192630767822,
      "learning_rate": 2.246376811594203e-05,
      "loss": 1.8052,
      "step": 2280
    },
    {
      "epoch": 8.278985507246377,
      "grad_norm": 2.780890464782715,
      "learning_rate": 2.240338164251208e-05,
      "loss": 1.8565,
      "step": 2285
    },
    {
      "epoch": 8.297101449275363,
      "grad_norm": 3.4677834510803223,
      "learning_rate": 2.2342995169082127e-05,
      "loss": 1.8129,
      "step": 2290
    },
    {
      "epoch": 8.315217391304348,
      "grad_norm": 3.154613494873047,
      "learning_rate": 2.2282608695652175e-05,
      "loss": 1.8004,
      "step": 2295
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 3.558716297149658,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 1.8695,
      "step": 2300
    },
    {
      "epoch": 8.351449275362318,
      "grad_norm": 3.05391526222229,
      "learning_rate": 2.216183574879227e-05,
      "loss": 1.8095,
      "step": 2305
    },
    {
      "epoch": 8.369565217391305,
      "grad_norm": 2.6697182655334473,
      "learning_rate": 2.2101449275362323e-05,
      "loss": 1.8037,
      "step": 2310
    },
    {
      "epoch": 8.38768115942029,
      "grad_norm": 3.437220811843872,
      "learning_rate": 2.204106280193237e-05,
      "loss": 1.8431,
      "step": 2315
    },
    {
      "epoch": 8.405797101449275,
      "grad_norm": 2.948153495788574,
      "learning_rate": 2.198067632850242e-05,
      "loss": 1.8859,
      "step": 2320
    },
    {
      "epoch": 8.423913043478262,
      "grad_norm": 3.0553953647613525,
      "learning_rate": 2.1920289855072466e-05,
      "loss": 1.9482,
      "step": 2325
    },
    {
      "epoch": 8.442028985507246,
      "grad_norm": 3.4141359329223633,
      "learning_rate": 2.1859903381642514e-05,
      "loss": 1.8919,
      "step": 2330
    },
    {
      "epoch": 8.460144927536232,
      "grad_norm": 3.0681917667388916,
      "learning_rate": 2.1799516908212562e-05,
      "loss": 1.8047,
      "step": 2335
    },
    {
      "epoch": 8.478260869565217,
      "grad_norm": 3.139883279800415,
      "learning_rate": 2.173913043478261e-05,
      "loss": 1.8527,
      "step": 2340
    },
    {
      "epoch": 8.496376811594203,
      "grad_norm": 2.874795913696289,
      "learning_rate": 2.1678743961352658e-05,
      "loss": 1.8548,
      "step": 2345
    },
    {
      "epoch": 8.514492753623188,
      "grad_norm": 2.837407112121582,
      "learning_rate": 2.1618357487922706e-05,
      "loss": 1.8477,
      "step": 2350
    },
    {
      "epoch": 8.532608695652174,
      "grad_norm": 3.285184860229492,
      "learning_rate": 2.1557971014492757e-05,
      "loss": 1.8223,
      "step": 2355
    },
    {
      "epoch": 8.55072463768116,
      "grad_norm": 3.6092207431793213,
      "learning_rate": 2.1497584541062805e-05,
      "loss": 1.5205,
      "step": 2360
    },
    {
      "epoch": 8.568840579710145,
      "grad_norm": 3.1757404804229736,
      "learning_rate": 2.1437198067632853e-05,
      "loss": 1.8027,
      "step": 2365
    },
    {
      "epoch": 8.58695652173913,
      "grad_norm": 3.35093092918396,
      "learning_rate": 2.13768115942029e-05,
      "loss": 1.8207,
      "step": 2370
    },
    {
      "epoch": 8.605072463768115,
      "grad_norm": 3.1027793884277344,
      "learning_rate": 2.131642512077295e-05,
      "loss": 1.8909,
      "step": 2375
    },
    {
      "epoch": 8.623188405797102,
      "grad_norm": 3.0731046199798584,
      "learning_rate": 2.1256038647342997e-05,
      "loss": 1.8566,
      "step": 2380
    },
    {
      "epoch": 8.641304347826086,
      "grad_norm": 3.2357356548309326,
      "learning_rate": 2.1195652173913045e-05,
      "loss": 1.7969,
      "step": 2385
    },
    {
      "epoch": 8.659420289855072,
      "grad_norm": 3.2031455039978027,
      "learning_rate": 2.1135265700483093e-05,
      "loss": 1.8782,
      "step": 2390
    },
    {
      "epoch": 8.677536231884059,
      "grad_norm": 2.7062251567840576,
      "learning_rate": 2.107487922705314e-05,
      "loss": 1.8654,
      "step": 2395
    },
    {
      "epoch": 8.695652173913043,
      "grad_norm": 3.429295063018799,
      "learning_rate": 2.101449275362319e-05,
      "loss": 1.8505,
      "step": 2400
    },
    {
      "epoch": 8.71376811594203,
      "grad_norm": 2.9152841567993164,
      "learning_rate": 2.0954106280193237e-05,
      "loss": 1.8146,
      "step": 2405
    },
    {
      "epoch": 8.731884057971014,
      "grad_norm": 3.133709669113159,
      "learning_rate": 2.0893719806763288e-05,
      "loss": 1.8512,
      "step": 2410
    },
    {
      "epoch": 8.75,
      "grad_norm": 3.442171573638916,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 1.813,
      "step": 2415
    },
    {
      "epoch": 8.768115942028986,
      "grad_norm": 2.937253475189209,
      "learning_rate": 2.0772946859903384e-05,
      "loss": 1.8392,
      "step": 2420
    },
    {
      "epoch": 8.78623188405797,
      "grad_norm": 3.799644708633423,
      "learning_rate": 2.0712560386473432e-05,
      "loss": 1.7612,
      "step": 2425
    },
    {
      "epoch": 8.804347826086957,
      "grad_norm": 2.791929244995117,
      "learning_rate": 2.065217391304348e-05,
      "loss": 1.7782,
      "step": 2430
    },
    {
      "epoch": 8.822463768115941,
      "grad_norm": 3.208209753036499,
      "learning_rate": 2.0591787439613528e-05,
      "loss": 1.7896,
      "step": 2435
    },
    {
      "epoch": 8.840579710144928,
      "grad_norm": 3.5236423015594482,
      "learning_rate": 2.0531400966183576e-05,
      "loss": 1.8647,
      "step": 2440
    },
    {
      "epoch": 8.858695652173914,
      "grad_norm": 3.0415291786193848,
      "learning_rate": 2.0471014492753624e-05,
      "loss": 1.7909,
      "step": 2445
    },
    {
      "epoch": 8.876811594202898,
      "grad_norm": 3.8428738117218018,
      "learning_rate": 2.041062801932367e-05,
      "loss": 1.8201,
      "step": 2450
    },
    {
      "epoch": 8.894927536231885,
      "grad_norm": 2.6577775478363037,
      "learning_rate": 2.035024154589372e-05,
      "loss": 1.8085,
      "step": 2455
    },
    {
      "epoch": 8.91304347826087,
      "grad_norm": 3.52714204788208,
      "learning_rate": 2.028985507246377e-05,
      "loss": 1.8463,
      "step": 2460
    },
    {
      "epoch": 8.931159420289855,
      "grad_norm": 3.559110403060913,
      "learning_rate": 2.022946859903382e-05,
      "loss": 1.821,
      "step": 2465
    },
    {
      "epoch": 8.94927536231884,
      "grad_norm": 2.693535327911377,
      "learning_rate": 2.0169082125603867e-05,
      "loss": 1.7811,
      "step": 2470
    },
    {
      "epoch": 8.967391304347826,
      "grad_norm": 3.070274591445923,
      "learning_rate": 2.0108695652173915e-05,
      "loss": 1.7305,
      "step": 2475
    },
    {
      "epoch": 8.985507246376812,
      "grad_norm": 3.3747916221618652,
      "learning_rate": 2.0048309178743963e-05,
      "loss": 1.7807,
      "step": 2480
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8327596187591553,
      "eval_runtime": 2.5303,
      "eval_samples_per_second": 19.365,
      "eval_steps_per_second": 2.766,
      "step": 2484
    },
    {
      "epoch": 9.003623188405797,
      "grad_norm": 2.3921337127685547,
      "learning_rate": 1.998792270531401e-05,
      "loss": 1.7755,
      "step": 2485
    },
    {
      "epoch": 9.021739130434783,
      "grad_norm": 3.0547115802764893,
      "learning_rate": 1.992753623188406e-05,
      "loss": 1.816,
      "step": 2490
    },
    {
      "epoch": 9.039855072463768,
      "grad_norm": 2.9782633781433105,
      "learning_rate": 1.9867149758454106e-05,
      "loss": 1.7819,
      "step": 2495
    },
    {
      "epoch": 9.057971014492754,
      "grad_norm": 2.603688955307007,
      "learning_rate": 1.9806763285024154e-05,
      "loss": 1.7916,
      "step": 2500
    },
    {
      "epoch": 9.076086956521738,
      "grad_norm": 2.7946205139160156,
      "learning_rate": 1.9746376811594202e-05,
      "loss": 1.7354,
      "step": 2505
    },
    {
      "epoch": 9.094202898550725,
      "grad_norm": 3.3895788192749023,
      "learning_rate": 1.9685990338164254e-05,
      "loss": 1.8031,
      "step": 2510
    },
    {
      "epoch": 9.11231884057971,
      "grad_norm": 3.206587314605713,
      "learning_rate": 1.96256038647343e-05,
      "loss": 1.8242,
      "step": 2515
    },
    {
      "epoch": 9.130434782608695,
      "grad_norm": 3.1063222885131836,
      "learning_rate": 1.956521739130435e-05,
      "loss": 1.8283,
      "step": 2520
    },
    {
      "epoch": 9.148550724637682,
      "grad_norm": 2.875488519668579,
      "learning_rate": 1.9504830917874397e-05,
      "loss": 1.8346,
      "step": 2525
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 3.1096463203430176,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 1.7886,
      "step": 2530
    },
    {
      "epoch": 9.184782608695652,
      "grad_norm": 3.88265061378479,
      "learning_rate": 1.9384057971014493e-05,
      "loss": 1.8505,
      "step": 2535
    },
    {
      "epoch": 9.202898550724637,
      "grad_norm": 3.9268925189971924,
      "learning_rate": 1.932367149758454e-05,
      "loss": 1.8327,
      "step": 2540
    },
    {
      "epoch": 9.221014492753623,
      "grad_norm": 3.833374500274658,
      "learning_rate": 1.926328502415459e-05,
      "loss": 1.8218,
      "step": 2545
    },
    {
      "epoch": 9.23913043478261,
      "grad_norm": 4.032691955566406,
      "learning_rate": 1.9202898550724637e-05,
      "loss": 1.8542,
      "step": 2550
    },
    {
      "epoch": 9.257246376811594,
      "grad_norm": 2.946209192276001,
      "learning_rate": 1.9142512077294685e-05,
      "loss": 1.8344,
      "step": 2555
    },
    {
      "epoch": 9.27536231884058,
      "grad_norm": 3.2425107955932617,
      "learning_rate": 1.9082125603864733e-05,
      "loss": 1.7911,
      "step": 2560
    },
    {
      "epoch": 9.293478260869565,
      "grad_norm": 2.910764694213867,
      "learning_rate": 1.9021739130434784e-05,
      "loss": 1.7855,
      "step": 2565
    },
    {
      "epoch": 9.31159420289855,
      "grad_norm": 3.717752456665039,
      "learning_rate": 1.8961352657004832e-05,
      "loss": 1.8087,
      "step": 2570
    },
    {
      "epoch": 9.329710144927537,
      "grad_norm": 2.6649413108825684,
      "learning_rate": 1.890096618357488e-05,
      "loss": 1.7886,
      "step": 2575
    },
    {
      "epoch": 9.347826086956522,
      "grad_norm": 4.042137622833252,
      "learning_rate": 1.8840579710144928e-05,
      "loss": 1.7485,
      "step": 2580
    },
    {
      "epoch": 9.365942028985508,
      "grad_norm": 3.6869635581970215,
      "learning_rate": 1.8780193236714976e-05,
      "loss": 1.8455,
      "step": 2585
    },
    {
      "epoch": 9.384057971014492,
      "grad_norm": 2.9332449436187744,
      "learning_rate": 1.8719806763285024e-05,
      "loss": 1.7972,
      "step": 2590
    },
    {
      "epoch": 9.402173913043478,
      "grad_norm": 3.2767951488494873,
      "learning_rate": 1.8659420289855072e-05,
      "loss": 1.8654,
      "step": 2595
    },
    {
      "epoch": 9.420289855072463,
      "grad_norm": 2.8652186393737793,
      "learning_rate": 1.859903381642512e-05,
      "loss": 1.7743,
      "step": 2600
    },
    {
      "epoch": 9.43840579710145,
      "grad_norm": 3.9237802028656006,
      "learning_rate": 1.8538647342995168e-05,
      "loss": 1.7976,
      "step": 2605
    },
    {
      "epoch": 9.456521739130435,
      "grad_norm": 2.8643479347229004,
      "learning_rate": 1.8478260869565216e-05,
      "loss": 1.8103,
      "step": 2610
    },
    {
      "epoch": 9.47463768115942,
      "grad_norm": 3.2659573554992676,
      "learning_rate": 1.8417874396135267e-05,
      "loss": 1.865,
      "step": 2615
    },
    {
      "epoch": 9.492753623188406,
      "grad_norm": 3.4077277183532715,
      "learning_rate": 1.8357487922705315e-05,
      "loss": 1.518,
      "step": 2620
    },
    {
      "epoch": 9.51086956521739,
      "grad_norm": 3.4026217460632324,
      "learning_rate": 1.8297101449275363e-05,
      "loss": 1.7765,
      "step": 2625
    },
    {
      "epoch": 9.528985507246377,
      "grad_norm": 3.3280484676361084,
      "learning_rate": 1.823671497584541e-05,
      "loss": 1.8057,
      "step": 2630
    },
    {
      "epoch": 9.547101449275363,
      "grad_norm": 3.7251601219177246,
      "learning_rate": 1.817632850241546e-05,
      "loss": 1.8916,
      "step": 2635
    },
    {
      "epoch": 9.565217391304348,
      "grad_norm": 3.8370416164398193,
      "learning_rate": 1.8115942028985507e-05,
      "loss": 1.8085,
      "step": 2640
    },
    {
      "epoch": 9.583333333333334,
      "grad_norm": 3.813490867614746,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 1.7519,
      "step": 2645
    },
    {
      "epoch": 9.601449275362318,
      "grad_norm": 3.791412353515625,
      "learning_rate": 1.7995169082125603e-05,
      "loss": 1.8034,
      "step": 2650
    },
    {
      "epoch": 9.619565217391305,
      "grad_norm": 3.3601326942443848,
      "learning_rate": 1.793478260869565e-05,
      "loss": 1.8583,
      "step": 2655
    },
    {
      "epoch": 9.63768115942029,
      "grad_norm": 2.9294328689575195,
      "learning_rate": 1.78743961352657e-05,
      "loss": 1.85,
      "step": 2660
    },
    {
      "epoch": 9.655797101449275,
      "grad_norm": 2.9844765663146973,
      "learning_rate": 1.781400966183575e-05,
      "loss": 1.7735,
      "step": 2665
    },
    {
      "epoch": 9.673913043478262,
      "grad_norm": 4.039002895355225,
      "learning_rate": 1.7753623188405798e-05,
      "loss": 1.8016,
      "step": 2670
    },
    {
      "epoch": 9.692028985507246,
      "grad_norm": 2.70034122467041,
      "learning_rate": 1.7693236714975846e-05,
      "loss": 1.8208,
      "step": 2675
    },
    {
      "epoch": 9.710144927536232,
      "grad_norm": 3.372101068496704,
      "learning_rate": 1.7632850241545894e-05,
      "loss": 1.8597,
      "step": 2680
    },
    {
      "epoch": 9.728260869565217,
      "grad_norm": 3.334291458129883,
      "learning_rate": 1.757246376811594e-05,
      "loss": 1.8459,
      "step": 2685
    },
    {
      "epoch": 9.746376811594203,
      "grad_norm": 2.9768359661102295,
      "learning_rate": 1.751207729468599e-05,
      "loss": 1.7631,
      "step": 2690
    },
    {
      "epoch": 9.764492753623188,
      "grad_norm": 3.3614046573638916,
      "learning_rate": 1.745169082125604e-05,
      "loss": 1.8477,
      "step": 2695
    },
    {
      "epoch": 9.782608695652174,
      "grad_norm": 3.0866646766662598,
      "learning_rate": 1.739130434782609e-05,
      "loss": 1.7431,
      "step": 2700
    },
    {
      "epoch": 9.80072463768116,
      "grad_norm": 3.5858540534973145,
      "learning_rate": 1.7330917874396137e-05,
      "loss": 1.8547,
      "step": 2705
    },
    {
      "epoch": 9.818840579710145,
      "grad_norm": 3.3101985454559326,
      "learning_rate": 1.7270531400966185e-05,
      "loss": 1.8015,
      "step": 2710
    },
    {
      "epoch": 9.83695652173913,
      "grad_norm": 2.889066457748413,
      "learning_rate": 1.7210144927536233e-05,
      "loss": 1.7826,
      "step": 2715
    },
    {
      "epoch": 9.855072463768115,
      "grad_norm": 2.9505093097686768,
      "learning_rate": 1.714975845410628e-05,
      "loss": 1.8103,
      "step": 2720
    },
    {
      "epoch": 9.873188405797102,
      "grad_norm": 2.9920523166656494,
      "learning_rate": 1.7089371980676332e-05,
      "loss": 1.8129,
      "step": 2725
    },
    {
      "epoch": 9.891304347826086,
      "grad_norm": 3.203155279159546,
      "learning_rate": 1.702898550724638e-05,
      "loss": 1.7933,
      "step": 2730
    },
    {
      "epoch": 9.909420289855072,
      "grad_norm": 3.1977105140686035,
      "learning_rate": 1.6968599033816428e-05,
      "loss": 1.7698,
      "step": 2735
    },
    {
      "epoch": 9.927536231884059,
      "grad_norm": 3.289371967315674,
      "learning_rate": 1.6908212560386476e-05,
      "loss": 1.7629,
      "step": 2740
    },
    {
      "epoch": 9.945652173913043,
      "grad_norm": 2.9992241859436035,
      "learning_rate": 1.6847826086956524e-05,
      "loss": 1.8627,
      "step": 2745
    },
    {
      "epoch": 9.96376811594203,
      "grad_norm": 3.2350854873657227,
      "learning_rate": 1.678743961352657e-05,
      "loss": 1.8005,
      "step": 2750
    },
    {
      "epoch": 9.981884057971014,
      "grad_norm": 2.4742588996887207,
      "learning_rate": 1.672705314009662e-05,
      "loss": 1.7834,
      "step": 2755
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.891024112701416,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.8433,
      "step": 2760
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8262642621994019,
      "eval_runtime": 2.4761,
      "eval_samples_per_second": 19.789,
      "eval_steps_per_second": 2.827,
      "step": 2760
    },
    {
      "epoch": 10.018115942028986,
      "grad_norm": 3.0154802799224854,
      "learning_rate": 1.6606280193236715e-05,
      "loss": 1.8228,
      "step": 2765
    },
    {
      "epoch": 10.03623188405797,
      "grad_norm": 3.322774648666382,
      "learning_rate": 1.6545893719806767e-05,
      "loss": 1.8813,
      "step": 2770
    },
    {
      "epoch": 10.054347826086957,
      "grad_norm": 3.33086895942688,
      "learning_rate": 1.6485507246376815e-05,
      "loss": 1.8321,
      "step": 2775
    },
    {
      "epoch": 10.072463768115941,
      "grad_norm": 2.6987380981445312,
      "learning_rate": 1.6425120772946863e-05,
      "loss": 1.8093,
      "step": 2780
    },
    {
      "epoch": 10.090579710144928,
      "grad_norm": 3.476684808731079,
      "learning_rate": 1.636473429951691e-05,
      "loss": 1.7493,
      "step": 2785
    },
    {
      "epoch": 10.108695652173912,
      "grad_norm": 3.4359328746795654,
      "learning_rate": 1.630434782608696e-05,
      "loss": 1.762,
      "step": 2790
    },
    {
      "epoch": 10.126811594202898,
      "grad_norm": 3.514090061187744,
      "learning_rate": 1.6243961352657006e-05,
      "loss": 1.7575,
      "step": 2795
    },
    {
      "epoch": 10.144927536231885,
      "grad_norm": 2.9493064880371094,
      "learning_rate": 1.6183574879227054e-05,
      "loss": 1.8218,
      "step": 2800
    },
    {
      "epoch": 10.16304347826087,
      "grad_norm": 4.029655933380127,
      "learning_rate": 1.6123188405797102e-05,
      "loss": 1.6949,
      "step": 2805
    },
    {
      "epoch": 10.181159420289855,
      "grad_norm": 3.4483625888824463,
      "learning_rate": 1.606280193236715e-05,
      "loss": 1.7785,
      "step": 2810
    },
    {
      "epoch": 10.19927536231884,
      "grad_norm": 3.238612174987793,
      "learning_rate": 1.6002415458937198e-05,
      "loss": 1.7158,
      "step": 2815
    },
    {
      "epoch": 10.217391304347826,
      "grad_norm": 2.9919092655181885,
      "learning_rate": 1.5942028985507246e-05,
      "loss": 1.8054,
      "step": 2820
    },
    {
      "epoch": 10.235507246376812,
      "grad_norm": 3.917208433151245,
      "learning_rate": 1.5881642512077297e-05,
      "loss": 1.8139,
      "step": 2825
    },
    {
      "epoch": 10.253623188405797,
      "grad_norm": 3.2639358043670654,
      "learning_rate": 1.5821256038647345e-05,
      "loss": 1.8206,
      "step": 2830
    },
    {
      "epoch": 10.271739130434783,
      "grad_norm": 3.1357431411743164,
      "learning_rate": 1.5760869565217393e-05,
      "loss": 1.7802,
      "step": 2835
    },
    {
      "epoch": 10.289855072463768,
      "grad_norm": 3.28300142288208,
      "learning_rate": 1.570048309178744e-05,
      "loss": 1.7971,
      "step": 2840
    },
    {
      "epoch": 10.307971014492754,
      "grad_norm": 2.8844785690307617,
      "learning_rate": 1.564009661835749e-05,
      "loss": 1.7982,
      "step": 2845
    },
    {
      "epoch": 10.326086956521738,
      "grad_norm": 2.69657039642334,
      "learning_rate": 1.5579710144927537e-05,
      "loss": 1.768,
      "step": 2850
    },
    {
      "epoch": 10.344202898550725,
      "grad_norm": 3.1459882259368896,
      "learning_rate": 1.5519323671497585e-05,
      "loss": 1.8061,
      "step": 2855
    },
    {
      "epoch": 10.36231884057971,
      "grad_norm": 3.1609606742858887,
      "learning_rate": 1.5458937198067633e-05,
      "loss": 1.7519,
      "step": 2860
    },
    {
      "epoch": 10.380434782608695,
      "grad_norm": 3.0096843242645264,
      "learning_rate": 1.539855072463768e-05,
      "loss": 1.7473,
      "step": 2865
    },
    {
      "epoch": 10.398550724637682,
      "grad_norm": 3.5023560523986816,
      "learning_rate": 1.533816425120773e-05,
      "loss": 1.7688,
      "step": 2870
    },
    {
      "epoch": 10.416666666666666,
      "grad_norm": 3.8200490474700928,
      "learning_rate": 1.527777777777778e-05,
      "loss": 1.8443,
      "step": 2875
    },
    {
      "epoch": 10.434782608695652,
      "grad_norm": 3.353804349899292,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 1.8063,
      "step": 2880
    },
    {
      "epoch": 10.452898550724637,
      "grad_norm": 3.527533531188965,
      "learning_rate": 1.5157004830917876e-05,
      "loss": 1.8216,
      "step": 2885
    },
    {
      "epoch": 10.471014492753623,
      "grad_norm": 3.172697067260742,
      "learning_rate": 1.5096618357487924e-05,
      "loss": 1.7719,
      "step": 2890
    },
    {
      "epoch": 10.48913043478261,
      "grad_norm": 3.3634374141693115,
      "learning_rate": 1.5036231884057972e-05,
      "loss": 1.7715,
      "step": 2895
    },
    {
      "epoch": 10.507246376811594,
      "grad_norm": 3.6370179653167725,
      "learning_rate": 1.497584541062802e-05,
      "loss": 1.7976,
      "step": 2900
    },
    {
      "epoch": 10.52536231884058,
      "grad_norm": 3.190235137939453,
      "learning_rate": 1.4915458937198068e-05,
      "loss": 1.849,
      "step": 2905
    },
    {
      "epoch": 10.543478260869565,
      "grad_norm": 3.2469449043273926,
      "learning_rate": 1.4855072463768116e-05,
      "loss": 1.7768,
      "step": 2910
    },
    {
      "epoch": 10.56159420289855,
      "grad_norm": 2.7376794815063477,
      "learning_rate": 1.4794685990338164e-05,
      "loss": 1.7698,
      "step": 2915
    },
    {
      "epoch": 10.579710144927537,
      "grad_norm": 2.8817191123962402,
      "learning_rate": 1.4734299516908212e-05,
      "loss": 1.7894,
      "step": 2920
    },
    {
      "epoch": 10.597826086956522,
      "grad_norm": 3.2691569328308105,
      "learning_rate": 1.4673913043478263e-05,
      "loss": 1.8219,
      "step": 2925
    },
    {
      "epoch": 10.615942028985508,
      "grad_norm": 3.3725948333740234,
      "learning_rate": 1.4613526570048311e-05,
      "loss": 1.8914,
      "step": 2930
    },
    {
      "epoch": 10.634057971014492,
      "grad_norm": 3.1332995891571045,
      "learning_rate": 1.4553140096618359e-05,
      "loss": 1.7904,
      "step": 2935
    },
    {
      "epoch": 10.652173913043478,
      "grad_norm": 3.347313165664673,
      "learning_rate": 1.4492753623188407e-05,
      "loss": 1.7799,
      "step": 2940
    },
    {
      "epoch": 10.670289855072463,
      "grad_norm": 2.8500261306762695,
      "learning_rate": 1.4432367149758455e-05,
      "loss": 1.8195,
      "step": 2945
    },
    {
      "epoch": 10.68840579710145,
      "grad_norm": 3.22385311126709,
      "learning_rate": 1.4371980676328503e-05,
      "loss": 1.7944,
      "step": 2950
    },
    {
      "epoch": 10.706521739130435,
      "grad_norm": 3.011427164077759,
      "learning_rate": 1.431159420289855e-05,
      "loss": 1.8118,
      "step": 2955
    },
    {
      "epoch": 10.72463768115942,
      "grad_norm": 2.844373941421509,
      "learning_rate": 1.4251207729468599e-05,
      "loss": 1.8335,
      "step": 2960
    },
    {
      "epoch": 10.742753623188406,
      "grad_norm": 2.887047290802002,
      "learning_rate": 1.4190821256038646e-05,
      "loss": 1.8325,
      "step": 2965
    },
    {
      "epoch": 10.76086956521739,
      "grad_norm": 2.883439779281616,
      "learning_rate": 1.4130434782608694e-05,
      "loss": 1.5518,
      "step": 2970
    },
    {
      "epoch": 10.778985507246377,
      "grad_norm": 2.447772264480591,
      "learning_rate": 1.4070048309178744e-05,
      "loss": 1.8563,
      "step": 2975
    },
    {
      "epoch": 10.797101449275363,
      "grad_norm": 3.3859174251556396,
      "learning_rate": 1.4009661835748794e-05,
      "loss": 1.8037,
      "step": 2980
    },
    {
      "epoch": 10.815217391304348,
      "grad_norm": 3.1078858375549316,
      "learning_rate": 1.3949275362318842e-05,
      "loss": 1.798,
      "step": 2985
    },
    {
      "epoch": 10.833333333333334,
      "grad_norm": 3.513385534286499,
      "learning_rate": 1.388888888888889e-05,
      "loss": 1.7357,
      "step": 2990
    },
    {
      "epoch": 10.851449275362318,
      "grad_norm": 3.1698720455169678,
      "learning_rate": 1.3828502415458937e-05,
      "loss": 1.8056,
      "step": 2995
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 3.7307755947113037,
      "learning_rate": 1.3768115942028985e-05,
      "loss": 1.8173,
      "step": 3000
    },
    {
      "epoch": 10.88768115942029,
      "grad_norm": 3.2182071208953857,
      "learning_rate": 1.3707729468599035e-05,
      "loss": 1.8583,
      "step": 3005
    },
    {
      "epoch": 10.905797101449275,
      "grad_norm": 3.197641611099243,
      "learning_rate": 1.3647342995169083e-05,
      "loss": 1.7862,
      "step": 3010
    },
    {
      "epoch": 10.923913043478262,
      "grad_norm": 3.6496756076812744,
      "learning_rate": 1.3586956521739131e-05,
      "loss": 1.7749,
      "step": 3015
    },
    {
      "epoch": 10.942028985507246,
      "grad_norm": 3.0509066581726074,
      "learning_rate": 1.3526570048309179e-05,
      "loss": 1.7107,
      "step": 3020
    },
    {
      "epoch": 10.960144927536232,
      "grad_norm": 2.807924270629883,
      "learning_rate": 1.3466183574879227e-05,
      "loss": 1.7959,
      "step": 3025
    },
    {
      "epoch": 10.978260869565217,
      "grad_norm": 3.356750011444092,
      "learning_rate": 1.3405797101449276e-05,
      "loss": 1.7747,
      "step": 3030
    },
    {
      "epoch": 10.996376811594203,
      "grad_norm": 2.71537446975708,
      "learning_rate": 1.3345410628019326e-05,
      "loss": 1.7372,
      "step": 3035
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.818696141242981,
      "eval_runtime": 2.4881,
      "eval_samples_per_second": 19.693,
      "eval_steps_per_second": 2.813,
      "step": 3036
    },
    {
      "epoch": 11.014492753623188,
      "grad_norm": 2.9769904613494873,
      "learning_rate": 1.3285024154589374e-05,
      "loss": 1.744,
      "step": 3040
    },
    {
      "epoch": 11.032608695652174,
      "grad_norm": 3.3321523666381836,
      "learning_rate": 1.3224637681159422e-05,
      "loss": 1.7493,
      "step": 3045
    },
    {
      "epoch": 11.05072463768116,
      "grad_norm": 3.2610971927642822,
      "learning_rate": 1.316425120772947e-05,
      "loss": 1.7876,
      "step": 3050
    },
    {
      "epoch": 11.068840579710145,
      "grad_norm": 3.710920572280884,
      "learning_rate": 1.3103864734299518e-05,
      "loss": 1.5084,
      "step": 3055
    },
    {
      "epoch": 11.08695652173913,
      "grad_norm": 3.1218807697296143,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 1.8494,
      "step": 3060
    },
    {
      "epoch": 11.105072463768115,
      "grad_norm": 3.078608274459839,
      "learning_rate": 1.2983091787439614e-05,
      "loss": 1.7273,
      "step": 3065
    },
    {
      "epoch": 11.123188405797102,
      "grad_norm": 3.6283061504364014,
      "learning_rate": 1.2922705314009662e-05,
      "loss": 1.7963,
      "step": 3070
    },
    {
      "epoch": 11.141304347826088,
      "grad_norm": 3.150912046432495,
      "learning_rate": 1.286231884057971e-05,
      "loss": 1.8043,
      "step": 3075
    },
    {
      "epoch": 11.159420289855072,
      "grad_norm": 3.0150551795959473,
      "learning_rate": 1.2801932367149761e-05,
      "loss": 1.7532,
      "step": 3080
    },
    {
      "epoch": 11.177536231884059,
      "grad_norm": 3.1286306381225586,
      "learning_rate": 1.2741545893719809e-05,
      "loss": 1.817,
      "step": 3085
    },
    {
      "epoch": 11.195652173913043,
      "grad_norm": 3.25819993019104,
      "learning_rate": 1.2681159420289857e-05,
      "loss": 1.8284,
      "step": 3090
    },
    {
      "epoch": 11.21376811594203,
      "grad_norm": 2.751164436340332,
      "learning_rate": 1.2620772946859905e-05,
      "loss": 1.7532,
      "step": 3095
    },
    {
      "epoch": 11.231884057971014,
      "grad_norm": 3.7209489345550537,
      "learning_rate": 1.2560386473429953e-05,
      "loss": 1.8288,
      "step": 3100
    },
    {
      "epoch": 11.25,
      "grad_norm": 2.913410186767578,
      "learning_rate": 1.25e-05,
      "loss": 1.7899,
      "step": 3105
    },
    {
      "epoch": 11.268115942028986,
      "grad_norm": 2.889878034591675,
      "learning_rate": 1.2439613526570049e-05,
      "loss": 1.7723,
      "step": 3110
    },
    {
      "epoch": 11.28623188405797,
      "grad_norm": 3.064674139022827,
      "learning_rate": 1.2379227053140096e-05,
      "loss": 1.7532,
      "step": 3115
    },
    {
      "epoch": 11.304347826086957,
      "grad_norm": 2.979414463043213,
      "learning_rate": 1.2318840579710146e-05,
      "loss": 1.791,
      "step": 3120
    },
    {
      "epoch": 11.322463768115941,
      "grad_norm": 2.9322738647460938,
      "learning_rate": 1.2258454106280194e-05,
      "loss": 1.7571,
      "step": 3125
    },
    {
      "epoch": 11.340579710144928,
      "grad_norm": 2.7468395233154297,
      "learning_rate": 1.2198067632850242e-05,
      "loss": 1.7631,
      "step": 3130
    },
    {
      "epoch": 11.358695652173912,
      "grad_norm": 3.611619234085083,
      "learning_rate": 1.213768115942029e-05,
      "loss": 1.7766,
      "step": 3135
    },
    {
      "epoch": 11.376811594202898,
      "grad_norm": 4.684986591339111,
      "learning_rate": 1.2077294685990338e-05,
      "loss": 1.7882,
      "step": 3140
    },
    {
      "epoch": 11.394927536231885,
      "grad_norm": 4.207118034362793,
      "learning_rate": 1.2016908212560387e-05,
      "loss": 1.7699,
      "step": 3145
    },
    {
      "epoch": 11.41304347826087,
      "grad_norm": 3.5089263916015625,
      "learning_rate": 1.1956521739130435e-05,
      "loss": 1.7636,
      "step": 3150
    },
    {
      "epoch": 11.431159420289855,
      "grad_norm": 3.480329990386963,
      "learning_rate": 1.1896135265700483e-05,
      "loss": 1.8276,
      "step": 3155
    },
    {
      "epoch": 11.44927536231884,
      "grad_norm": 3.0863852500915527,
      "learning_rate": 1.1835748792270531e-05,
      "loss": 1.7524,
      "step": 3160
    },
    {
      "epoch": 11.467391304347826,
      "grad_norm": 2.803175926208496,
      "learning_rate": 1.177536231884058e-05,
      "loss": 1.8533,
      "step": 3165
    },
    {
      "epoch": 11.485507246376812,
      "grad_norm": 3.0040204524993896,
      "learning_rate": 1.1714975845410629e-05,
      "loss": 1.8167,
      "step": 3170
    },
    {
      "epoch": 11.503623188405797,
      "grad_norm": 3.7044930458068848,
      "learning_rate": 1.1654589371980677e-05,
      "loss": 1.76,
      "step": 3175
    },
    {
      "epoch": 11.521739130434783,
      "grad_norm": 3.3602006435394287,
      "learning_rate": 1.1594202898550725e-05,
      "loss": 1.7042,
      "step": 3180
    },
    {
      "epoch": 11.539855072463768,
      "grad_norm": 3.1315994262695312,
      "learning_rate": 1.1533816425120773e-05,
      "loss": 1.8322,
      "step": 3185
    },
    {
      "epoch": 11.557971014492754,
      "grad_norm": 3.397768974304199,
      "learning_rate": 1.147342995169082e-05,
      "loss": 1.7635,
      "step": 3190
    },
    {
      "epoch": 11.576086956521738,
      "grad_norm": 3.2959144115448,
      "learning_rate": 1.141304347826087e-05,
      "loss": 1.8017,
      "step": 3195
    },
    {
      "epoch": 11.594202898550725,
      "grad_norm": 3.235225200653076,
      "learning_rate": 1.1352657004830918e-05,
      "loss": 1.7953,
      "step": 3200
    },
    {
      "epoch": 11.61231884057971,
      "grad_norm": 2.588395357131958,
      "learning_rate": 1.1292270531400966e-05,
      "loss": 1.7702,
      "step": 3205
    },
    {
      "epoch": 11.630434782608695,
      "grad_norm": 2.719435453414917,
      "learning_rate": 1.1231884057971016e-05,
      "loss": 1.7137,
      "step": 3210
    },
    {
      "epoch": 11.648550724637682,
      "grad_norm": 3.710953712463379,
      "learning_rate": 1.1171497584541064e-05,
      "loss": 1.8456,
      "step": 3215
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 3.174804925918579,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 1.8057,
      "step": 3220
    },
    {
      "epoch": 11.684782608695652,
      "grad_norm": 3.253316879272461,
      "learning_rate": 1.1050724637681161e-05,
      "loss": 1.743,
      "step": 3225
    },
    {
      "epoch": 11.702898550724637,
      "grad_norm": 3.113515853881836,
      "learning_rate": 1.099033816425121e-05,
      "loss": 1.737,
      "step": 3230
    },
    {
      "epoch": 11.721014492753623,
      "grad_norm": 2.830106496810913,
      "learning_rate": 1.0929951690821257e-05,
      "loss": 1.7951,
      "step": 3235
    },
    {
      "epoch": 11.73913043478261,
      "grad_norm": 2.8535609245300293,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 1.8021,
      "step": 3240
    },
    {
      "epoch": 11.757246376811594,
      "grad_norm": 2.8670668601989746,
      "learning_rate": 1.0809178743961353e-05,
      "loss": 1.7639,
      "step": 3245
    },
    {
      "epoch": 11.77536231884058,
      "grad_norm": 3.0613911151885986,
      "learning_rate": 1.0748792270531403e-05,
      "loss": 1.7875,
      "step": 3250
    },
    {
      "epoch": 11.793478260869565,
      "grad_norm": 3.251964807510376,
      "learning_rate": 1.068840579710145e-05,
      "loss": 1.7498,
      "step": 3255
    },
    {
      "epoch": 11.81159420289855,
      "grad_norm": 2.87591814994812,
      "learning_rate": 1.0628019323671499e-05,
      "loss": 1.7959,
      "step": 3260
    },
    {
      "epoch": 11.829710144927537,
      "grad_norm": 2.992128372192383,
      "learning_rate": 1.0567632850241546e-05,
      "loss": 1.7472,
      "step": 3265
    },
    {
      "epoch": 11.847826086956522,
      "grad_norm": 2.9947071075439453,
      "learning_rate": 1.0507246376811594e-05,
      "loss": 1.8306,
      "step": 3270
    },
    {
      "epoch": 11.865942028985508,
      "grad_norm": 2.7505507469177246,
      "learning_rate": 1.0446859903381644e-05,
      "loss": 1.781,
      "step": 3275
    },
    {
      "epoch": 11.884057971014492,
      "grad_norm": 3.1160905361175537,
      "learning_rate": 1.0386473429951692e-05,
      "loss": 1.8184,
      "step": 3280
    },
    {
      "epoch": 11.902173913043478,
      "grad_norm": 3.3020036220550537,
      "learning_rate": 1.032608695652174e-05,
      "loss": 1.8034,
      "step": 3285
    },
    {
      "epoch": 11.920289855072463,
      "grad_norm": 3.445188522338867,
      "learning_rate": 1.0265700483091788e-05,
      "loss": 1.8133,
      "step": 3290
    },
    {
      "epoch": 11.93840579710145,
      "grad_norm": 3.3335702419281006,
      "learning_rate": 1.0205314009661836e-05,
      "loss": 1.7944,
      "step": 3295
    },
    {
      "epoch": 11.956521739130435,
      "grad_norm": 3.1112730503082275,
      "learning_rate": 1.0144927536231885e-05,
      "loss": 1.8082,
      "step": 3300
    },
    {
      "epoch": 11.97463768115942,
      "grad_norm": 3.2081711292266846,
      "learning_rate": 1.0084541062801933e-05,
      "loss": 1.7482,
      "step": 3305
    },
    {
      "epoch": 11.992753623188406,
      "grad_norm": 2.869706869125366,
      "learning_rate": 1.0024154589371981e-05,
      "loss": 1.7275,
      "step": 3310
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.821272611618042,
      "eval_runtime": 0.1503,
      "eval_samples_per_second": 325.97,
      "eval_steps_per_second": 46.567,
      "step": 3312
    },
    {
      "epoch": 12.01086956521739,
      "grad_norm": 2.9292514324188232,
      "learning_rate": 9.96376811594203e-06,
      "loss": 1.8089,
      "step": 3315
    },
    {
      "epoch": 12.028985507246377,
      "grad_norm": 2.796792984008789,
      "learning_rate": 9.903381642512077e-06,
      "loss": 1.8132,
      "step": 3320
    },
    {
      "epoch": 12.047101449275363,
      "grad_norm": 3.098687171936035,
      "learning_rate": 9.842995169082127e-06,
      "loss": 1.8334,
      "step": 3325
    },
    {
      "epoch": 12.065217391304348,
      "grad_norm": 3.0765380859375,
      "learning_rate": 9.782608695652175e-06,
      "loss": 1.7373,
      "step": 3330
    },
    {
      "epoch": 12.083333333333334,
      "grad_norm": 2.9989819526672363,
      "learning_rate": 9.722222222222223e-06,
      "loss": 1.808,
      "step": 3335
    },
    {
      "epoch": 12.101449275362318,
      "grad_norm": 3.7950522899627686,
      "learning_rate": 9.66183574879227e-06,
      "loss": 1.7737,
      "step": 3340
    },
    {
      "epoch": 12.119565217391305,
      "grad_norm": 2.796828269958496,
      "learning_rate": 9.601449275362319e-06,
      "loss": 1.738,
      "step": 3345
    },
    {
      "epoch": 12.13768115942029,
      "grad_norm": 3.141812562942505,
      "learning_rate": 9.541062801932367e-06,
      "loss": 1.8153,
      "step": 3350
    },
    {
      "epoch": 12.155797101449275,
      "grad_norm": 3.219088315963745,
      "learning_rate": 9.480676328502416e-06,
      "loss": 1.781,
      "step": 3355
    },
    {
      "epoch": 12.173913043478262,
      "grad_norm": 2.5183827877044678,
      "learning_rate": 9.420289855072464e-06,
      "loss": 1.7745,
      "step": 3360
    },
    {
      "epoch": 12.192028985507246,
      "grad_norm": 3.4610540866851807,
      "learning_rate": 9.359903381642512e-06,
      "loss": 1.744,
      "step": 3365
    },
    {
      "epoch": 12.210144927536232,
      "grad_norm": 3.1681320667266846,
      "learning_rate": 9.29951690821256e-06,
      "loss": 1.8016,
      "step": 3370
    },
    {
      "epoch": 12.228260869565217,
      "grad_norm": 3.0129127502441406,
      "learning_rate": 9.239130434782608e-06,
      "loss": 1.8377,
      "step": 3375
    },
    {
      "epoch": 12.246376811594203,
      "grad_norm": 3.4553773403167725,
      "learning_rate": 9.178743961352658e-06,
      "loss": 1.7773,
      "step": 3380
    },
    {
      "epoch": 12.264492753623188,
      "grad_norm": 3.555222511291504,
      "learning_rate": 9.118357487922705e-06,
      "loss": 1.842,
      "step": 3385
    },
    {
      "epoch": 12.282608695652174,
      "grad_norm": 3.0844099521636963,
      "learning_rate": 9.057971014492753e-06,
      "loss": 1.8154,
      "step": 3390
    },
    {
      "epoch": 12.30072463768116,
      "grad_norm": 3.136533260345459,
      "learning_rate": 8.997584541062801e-06,
      "loss": 1.7936,
      "step": 3395
    },
    {
      "epoch": 12.318840579710145,
      "grad_norm": 3.333200454711914,
      "learning_rate": 8.93719806763285e-06,
      "loss": 1.7959,
      "step": 3400
    },
    {
      "epoch": 12.33695652173913,
      "grad_norm": 2.8436198234558105,
      "learning_rate": 8.876811594202899e-06,
      "loss": 1.7459,
      "step": 3405
    },
    {
      "epoch": 12.355072463768115,
      "grad_norm": 2.784353017807007,
      "learning_rate": 8.816425120772947e-06,
      "loss": 1.6555,
      "step": 3410
    },
    {
      "epoch": 12.373188405797102,
      "grad_norm": 2.590506076812744,
      "learning_rate": 8.756038647342995e-06,
      "loss": 1.7922,
      "step": 3415
    },
    {
      "epoch": 12.391304347826088,
      "grad_norm": 3.047804355621338,
      "learning_rate": 8.695652173913044e-06,
      "loss": 1.788,
      "step": 3420
    },
    {
      "epoch": 12.409420289855072,
      "grad_norm": 2.8288867473602295,
      "learning_rate": 8.635265700483092e-06,
      "loss": 1.7916,
      "step": 3425
    },
    {
      "epoch": 12.427536231884059,
      "grad_norm": 3.482656240463257,
      "learning_rate": 8.57487922705314e-06,
      "loss": 1.7694,
      "step": 3430
    },
    {
      "epoch": 12.445652173913043,
      "grad_norm": 2.946058988571167,
      "learning_rate": 8.51449275362319e-06,
      "loss": 1.7552,
      "step": 3435
    },
    {
      "epoch": 12.46376811594203,
      "grad_norm": 3.5083537101745605,
      "learning_rate": 8.454106280193238e-06,
      "loss": 1.7914,
      "step": 3440
    },
    {
      "epoch": 12.481884057971014,
      "grad_norm": 3.2074382305145264,
      "learning_rate": 8.393719806763286e-06,
      "loss": 1.7592,
      "step": 3445
    },
    {
      "epoch": 12.5,
      "grad_norm": 3.118898630142212,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.8146,
      "step": 3450
    },
    {
      "epoch": 12.518115942028986,
      "grad_norm": 3.009335517883301,
      "learning_rate": 8.272946859903383e-06,
      "loss": 1.8137,
      "step": 3455
    },
    {
      "epoch": 12.53623188405797,
      "grad_norm": 2.9018449783325195,
      "learning_rate": 8.212560386473431e-06,
      "loss": 1.797,
      "step": 3460
    },
    {
      "epoch": 12.554347826086957,
      "grad_norm": 2.838229179382324,
      "learning_rate": 8.15217391304348e-06,
      "loss": 1.7434,
      "step": 3465
    },
    {
      "epoch": 12.572463768115941,
      "grad_norm": 3.4703850746154785,
      "learning_rate": 8.091787439613527e-06,
      "loss": 1.7451,
      "step": 3470
    },
    {
      "epoch": 12.590579710144928,
      "grad_norm": 2.7246992588043213,
      "learning_rate": 8.031400966183575e-06,
      "loss": 1.7244,
      "step": 3475
    },
    {
      "epoch": 12.608695652173914,
      "grad_norm": 2.7495620250701904,
      "learning_rate": 7.971014492753623e-06,
      "loss": 1.8281,
      "step": 3480
    },
    {
      "epoch": 12.626811594202898,
      "grad_norm": 3.189638614654541,
      "learning_rate": 7.910628019323673e-06,
      "loss": 1.7049,
      "step": 3485
    },
    {
      "epoch": 12.644927536231885,
      "grad_norm": 3.107598066329956,
      "learning_rate": 7.85024154589372e-06,
      "loss": 1.8225,
      "step": 3490
    },
    {
      "epoch": 12.66304347826087,
      "grad_norm": 3.2254927158355713,
      "learning_rate": 7.789855072463769e-06,
      "loss": 1.7291,
      "step": 3495
    },
    {
      "epoch": 12.681159420289855,
      "grad_norm": 3.250164747238159,
      "learning_rate": 7.729468599033817e-06,
      "loss": 1.7039,
      "step": 3500
    },
    {
      "epoch": 12.69927536231884,
      "grad_norm": 3.0707895755767822,
      "learning_rate": 7.669082125603864e-06,
      "loss": 1.5097,
      "step": 3505
    },
    {
      "epoch": 12.717391304347826,
      "grad_norm": 3.4292984008789062,
      "learning_rate": 7.608695652173914e-06,
      "loss": 1.757,
      "step": 3510
    },
    {
      "epoch": 12.735507246376812,
      "grad_norm": 3.799537181854248,
      "learning_rate": 7.548309178743962e-06,
      "loss": 1.7954,
      "step": 3515
    },
    {
      "epoch": 12.753623188405797,
      "grad_norm": 3.4609575271606445,
      "learning_rate": 7.48792270531401e-06,
      "loss": 1.8041,
      "step": 3520
    },
    {
      "epoch": 12.771739130434783,
      "grad_norm": 2.4847447872161865,
      "learning_rate": 7.427536231884058e-06,
      "loss": 1.6771,
      "step": 3525
    },
    {
      "epoch": 12.789855072463768,
      "grad_norm": 2.4524221420288086,
      "learning_rate": 7.367149758454106e-06,
      "loss": 1.7591,
      "step": 3530
    },
    {
      "epoch": 12.807971014492754,
      "grad_norm": 3.7714693546295166,
      "learning_rate": 7.3067632850241555e-06,
      "loss": 1.8091,
      "step": 3535
    },
    {
      "epoch": 12.826086956521738,
      "grad_norm": 3.4557418823242188,
      "learning_rate": 7.246376811594203e-06,
      "loss": 1.7938,
      "step": 3540
    },
    {
      "epoch": 12.844202898550725,
      "grad_norm": 3.0014760494232178,
      "learning_rate": 7.185990338164251e-06,
      "loss": 1.7537,
      "step": 3545
    },
    {
      "epoch": 12.86231884057971,
      "grad_norm": 3.0227138996124268,
      "learning_rate": 7.125603864734299e-06,
      "loss": 1.7717,
      "step": 3550
    },
    {
      "epoch": 12.880434782608695,
      "grad_norm": 3.150300979614258,
      "learning_rate": 7.065217391304347e-06,
      "loss": 1.7634,
      "step": 3555
    },
    {
      "epoch": 12.898550724637682,
      "grad_norm": 3.134105920791626,
      "learning_rate": 7.004830917874397e-06,
      "loss": 1.7304,
      "step": 3560
    },
    {
      "epoch": 12.916666666666666,
      "grad_norm": 3.1089277267456055,
      "learning_rate": 6.944444444444445e-06,
      "loss": 1.7454,
      "step": 3565
    },
    {
      "epoch": 12.934782608695652,
      "grad_norm": 2.9496870040893555,
      "learning_rate": 6.884057971014493e-06,
      "loss": 1.7639,
      "step": 3570
    },
    {
      "epoch": 12.952898550724637,
      "grad_norm": 2.9439537525177,
      "learning_rate": 6.8236714975845415e-06,
      "loss": 1.7846,
      "step": 3575
    },
    {
      "epoch": 12.971014492753623,
      "grad_norm": 2.9986915588378906,
      "learning_rate": 6.7632850241545894e-06,
      "loss": 1.7519,
      "step": 3580
    },
    {
      "epoch": 12.98913043478261,
      "grad_norm": 3.1403629779815674,
      "learning_rate": 6.702898550724638e-06,
      "loss": 1.7562,
      "step": 3585
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8176957368850708,
      "eval_runtime": 2.4147,
      "eval_samples_per_second": 20.292,
      "eval_steps_per_second": 2.899,
      "step": 3588
    },
    {
      "epoch": 13.007246376811594,
      "grad_norm": 2.730844259262085,
      "learning_rate": 6.642512077294687e-06,
      "loss": 1.8542,
      "step": 3590
    },
    {
      "epoch": 13.02536231884058,
      "grad_norm": 3.3166067600250244,
      "learning_rate": 6.582125603864735e-06,
      "loss": 1.7703,
      "step": 3595
    },
    {
      "epoch": 13.043478260869565,
      "grad_norm": 2.6592934131622314,
      "learning_rate": 6.521739130434783e-06,
      "loss": 1.7853,
      "step": 3600
    },
    {
      "epoch": 13.06159420289855,
      "grad_norm": 3.077211618423462,
      "learning_rate": 6.461352657004831e-06,
      "loss": 1.7724,
      "step": 3605
    },
    {
      "epoch": 13.079710144927537,
      "grad_norm": 3.216228485107422,
      "learning_rate": 6.4009661835748805e-06,
      "loss": 1.7701,
      "step": 3610
    },
    {
      "epoch": 13.097826086956522,
      "grad_norm": 2.9226465225219727,
      "learning_rate": 6.340579710144928e-06,
      "loss": 1.7149,
      "step": 3615
    },
    {
      "epoch": 13.115942028985508,
      "grad_norm": 3.3250012397766113,
      "learning_rate": 6.280193236714976e-06,
      "loss": 1.787,
      "step": 3620
    },
    {
      "epoch": 13.134057971014492,
      "grad_norm": 2.9099795818328857,
      "learning_rate": 6.219806763285024e-06,
      "loss": 1.7755,
      "step": 3625
    },
    {
      "epoch": 13.152173913043478,
      "grad_norm": 2.8620145320892334,
      "learning_rate": 6.159420289855073e-06,
      "loss": 1.7268,
      "step": 3630
    },
    {
      "epoch": 13.170289855072463,
      "grad_norm": 3.1747546195983887,
      "learning_rate": 6.099033816425121e-06,
      "loss": 1.8145,
      "step": 3635
    },
    {
      "epoch": 13.18840579710145,
      "grad_norm": 3.1807475090026855,
      "learning_rate": 6.038647342995169e-06,
      "loss": 1.7938,
      "step": 3640
    },
    {
      "epoch": 13.206521739130435,
      "grad_norm": 3.533423662185669,
      "learning_rate": 5.978260869565218e-06,
      "loss": 1.7441,
      "step": 3645
    },
    {
      "epoch": 13.22463768115942,
      "grad_norm": 2.8345460891723633,
      "learning_rate": 5.917874396135266e-06,
      "loss": 1.7559,
      "step": 3650
    },
    {
      "epoch": 13.242753623188406,
      "grad_norm": 3.072275161743164,
      "learning_rate": 5.8574879227053144e-06,
      "loss": 1.7083,
      "step": 3655
    },
    {
      "epoch": 13.26086956521739,
      "grad_norm": 3.078465223312378,
      "learning_rate": 5.797101449275362e-06,
      "loss": 1.7282,
      "step": 3660
    },
    {
      "epoch": 13.278985507246377,
      "grad_norm": 2.796431541442871,
      "learning_rate": 5.73671497584541e-06,
      "loss": 1.7554,
      "step": 3665
    },
    {
      "epoch": 13.297101449275363,
      "grad_norm": 3.6359055042266846,
      "learning_rate": 5.676328502415459e-06,
      "loss": 1.6839,
      "step": 3670
    },
    {
      "epoch": 13.315217391304348,
      "grad_norm": 2.8123490810394287,
      "learning_rate": 5.615942028985508e-06,
      "loss": 1.7332,
      "step": 3675
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 3.121549367904663,
      "learning_rate": 5.555555555555556e-06,
      "loss": 1.5246,
      "step": 3680
    },
    {
      "epoch": 13.351449275362318,
      "grad_norm": 2.9972422122955322,
      "learning_rate": 5.495169082125605e-06,
      "loss": 1.7247,
      "step": 3685
    },
    {
      "epoch": 13.369565217391305,
      "grad_norm": 6.3965559005737305,
      "learning_rate": 5.4347826086956525e-06,
      "loss": 1.7842,
      "step": 3690
    },
    {
      "epoch": 13.38768115942029,
      "grad_norm": 2.9965639114379883,
      "learning_rate": 5.374396135265701e-06,
      "loss": 1.8111,
      "step": 3695
    },
    {
      "epoch": 13.405797101449275,
      "grad_norm": 3.2840609550476074,
      "learning_rate": 5.314009661835749e-06,
      "loss": 1.7594,
      "step": 3700
    },
    {
      "epoch": 13.423913043478262,
      "grad_norm": 3.2514662742614746,
      "learning_rate": 5.253623188405797e-06,
      "loss": 1.8244,
      "step": 3705
    },
    {
      "epoch": 13.442028985507246,
      "grad_norm": 3.229184627532959,
      "learning_rate": 5.193236714975846e-06,
      "loss": 1.7454,
      "step": 3710
    },
    {
      "epoch": 13.460144927536232,
      "grad_norm": 3.329817056655884,
      "learning_rate": 5.132850241545894e-06,
      "loss": 1.7424,
      "step": 3715
    },
    {
      "epoch": 13.478260869565217,
      "grad_norm": 3.4590868949890137,
      "learning_rate": 5.072463768115943e-06,
      "loss": 1.7943,
      "step": 3720
    },
    {
      "epoch": 13.496376811594203,
      "grad_norm": 2.9640746116638184,
      "learning_rate": 5.012077294685991e-06,
      "loss": 1.7764,
      "step": 3725
    },
    {
      "epoch": 13.514492753623188,
      "grad_norm": 2.919370412826538,
      "learning_rate": 4.951690821256039e-06,
      "loss": 1.7218,
      "step": 3730
    },
    {
      "epoch": 13.532608695652174,
      "grad_norm": 3.258835792541504,
      "learning_rate": 4.891304347826087e-06,
      "loss": 1.7999,
      "step": 3735
    },
    {
      "epoch": 13.55072463768116,
      "grad_norm": 2.820352077484131,
      "learning_rate": 4.830917874396135e-06,
      "loss": 1.8124,
      "step": 3740
    },
    {
      "epoch": 13.568840579710145,
      "grad_norm": 2.9134786128997803,
      "learning_rate": 4.770531400966183e-06,
      "loss": 1.7644,
      "step": 3745
    },
    {
      "epoch": 13.58695652173913,
      "grad_norm": 3.3281118869781494,
      "learning_rate": 4.710144927536232e-06,
      "loss": 1.7349,
      "step": 3750
    },
    {
      "epoch": 13.605072463768115,
      "grad_norm": 2.9983649253845215,
      "learning_rate": 4.64975845410628e-06,
      "loss": 1.7984,
      "step": 3755
    },
    {
      "epoch": 13.623188405797102,
      "grad_norm": 2.931448459625244,
      "learning_rate": 4.589371980676329e-06,
      "loss": 1.7736,
      "step": 3760
    },
    {
      "epoch": 13.641304347826086,
      "grad_norm": 2.9726431369781494,
      "learning_rate": 4.528985507246377e-06,
      "loss": 1.7101,
      "step": 3765
    },
    {
      "epoch": 13.659420289855072,
      "grad_norm": 3.4019381999969482,
      "learning_rate": 4.468599033816425e-06,
      "loss": 1.7932,
      "step": 3770
    },
    {
      "epoch": 13.677536231884059,
      "grad_norm": 3.038886070251465,
      "learning_rate": 4.408212560386473e-06,
      "loss": 1.7138,
      "step": 3775
    },
    {
      "epoch": 13.695652173913043,
      "grad_norm": 3.1820104122161865,
      "learning_rate": 4.347826086956522e-06,
      "loss": 1.75,
      "step": 3780
    },
    {
      "epoch": 13.71376811594203,
      "grad_norm": 2.8698291778564453,
      "learning_rate": 4.28743961352657e-06,
      "loss": 1.7527,
      "step": 3785
    },
    {
      "epoch": 13.731884057971014,
      "grad_norm": 2.4587204456329346,
      "learning_rate": 4.227053140096619e-06,
      "loss": 1.794,
      "step": 3790
    },
    {
      "epoch": 13.75,
      "grad_norm": 2.968336343765259,
      "learning_rate": 4.166666666666667e-06,
      "loss": 1.7751,
      "step": 3795
    },
    {
      "epoch": 13.768115942028986,
      "grad_norm": 2.9061198234558105,
      "learning_rate": 4.106280193236716e-06,
      "loss": 1.7728,
      "step": 3800
    },
    {
      "epoch": 13.78623188405797,
      "grad_norm": 3.0170717239379883,
      "learning_rate": 4.045893719806764e-06,
      "loss": 1.7791,
      "step": 3805
    },
    {
      "epoch": 13.804347826086957,
      "grad_norm": 3.6135294437408447,
      "learning_rate": 3.9855072463768115e-06,
      "loss": 1.795,
      "step": 3810
    },
    {
      "epoch": 13.822463768115941,
      "grad_norm": 3.3090336322784424,
      "learning_rate": 3.92512077294686e-06,
      "loss": 1.7089,
      "step": 3815
    },
    {
      "epoch": 13.840579710144928,
      "grad_norm": 3.7746310234069824,
      "learning_rate": 3.864734299516908e-06,
      "loss": 1.7504,
      "step": 3820
    },
    {
      "epoch": 13.858695652173914,
      "grad_norm": 2.8412866592407227,
      "learning_rate": 3.804347826086957e-06,
      "loss": 1.7926,
      "step": 3825
    },
    {
      "epoch": 13.876811594202898,
      "grad_norm": 3.578739643096924,
      "learning_rate": 3.743961352657005e-06,
      "loss": 1.7306,
      "step": 3830
    },
    {
      "epoch": 13.894927536231885,
      "grad_norm": 3.4075734615325928,
      "learning_rate": 3.683574879227053e-06,
      "loss": 1.7793,
      "step": 3835
    },
    {
      "epoch": 13.91304347826087,
      "grad_norm": 3.2662911415100098,
      "learning_rate": 3.6231884057971017e-06,
      "loss": 1.7625,
      "step": 3840
    },
    {
      "epoch": 13.931159420289855,
      "grad_norm": 2.838656187057495,
      "learning_rate": 3.5628019323671496e-06,
      "loss": 1.7375,
      "step": 3845
    },
    {
      "epoch": 13.94927536231884,
      "grad_norm": 2.987394332885742,
      "learning_rate": 3.5024154589371984e-06,
      "loss": 1.81,
      "step": 3850
    },
    {
      "epoch": 13.967391304347826,
      "grad_norm": 3.232152223587036,
      "learning_rate": 3.4420289855072464e-06,
      "loss": 1.804,
      "step": 3855
    },
    {
      "epoch": 13.985507246376812,
      "grad_norm": 3.299774408340454,
      "learning_rate": 3.3816425120772947e-06,
      "loss": 1.7874,
      "step": 3860
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8162930011749268,
      "eval_runtime": 3.4637,
      "eval_samples_per_second": 14.147,
      "eval_steps_per_second": 2.021,
      "step": 3864
    },
    {
      "epoch": 14.003623188405797,
      "grad_norm": 2.5408506393432617,
      "learning_rate": 3.3212560386473435e-06,
      "loss": 1.7015,
      "step": 3865
    },
    {
      "epoch": 14.021739130434783,
      "grad_norm": 3.2047388553619385,
      "learning_rate": 3.2608695652173914e-06,
      "loss": 1.6924,
      "step": 3870
    },
    {
      "epoch": 14.039855072463768,
      "grad_norm": 2.8191585540771484,
      "learning_rate": 3.2004830917874402e-06,
      "loss": 1.7504,
      "step": 3875
    },
    {
      "epoch": 14.057971014492754,
      "grad_norm": 2.9319138526916504,
      "learning_rate": 3.140096618357488e-06,
      "loss": 1.7385,
      "step": 3880
    },
    {
      "epoch": 14.076086956521738,
      "grad_norm": 2.694493055343628,
      "learning_rate": 3.0797101449275365e-06,
      "loss": 1.7659,
      "step": 3885
    },
    {
      "epoch": 14.094202898550725,
      "grad_norm": 3.588507652282715,
      "learning_rate": 3.0193236714975845e-06,
      "loss": 1.7388,
      "step": 3890
    },
    {
      "epoch": 14.11231884057971,
      "grad_norm": 3.0720691680908203,
      "learning_rate": 2.958937198067633e-06,
      "loss": 1.7634,
      "step": 3895
    },
    {
      "epoch": 14.130434782608695,
      "grad_norm": 3.1621148586273193,
      "learning_rate": 2.898550724637681e-06,
      "loss": 1.8197,
      "step": 3900
    },
    {
      "epoch": 14.148550724637682,
      "grad_norm": 3.2208197116851807,
      "learning_rate": 2.8381642512077295e-06,
      "loss": 1.7747,
      "step": 3905
    },
    {
      "epoch": 14.166666666666666,
      "grad_norm": 3.0025999546051025,
      "learning_rate": 2.777777777777778e-06,
      "loss": 1.7998,
      "step": 3910
    },
    {
      "epoch": 14.184782608695652,
      "grad_norm": 3.4129605293273926,
      "learning_rate": 2.7173913043478263e-06,
      "loss": 1.8072,
      "step": 3915
    },
    {
      "epoch": 14.202898550724637,
      "grad_norm": 3.3287746906280518,
      "learning_rate": 2.6570048309178746e-06,
      "loss": 1.7439,
      "step": 3920
    },
    {
      "epoch": 14.221014492753623,
      "grad_norm": 2.8053481578826904,
      "learning_rate": 2.596618357487923e-06,
      "loss": 1.7903,
      "step": 3925
    },
    {
      "epoch": 14.23913043478261,
      "grad_norm": 3.418156147003174,
      "learning_rate": 2.5362318840579714e-06,
      "loss": 1.7411,
      "step": 3930
    },
    {
      "epoch": 14.257246376811594,
      "grad_norm": 2.6508872509002686,
      "learning_rate": 2.4758454106280193e-06,
      "loss": 1.7937,
      "step": 3935
    },
    {
      "epoch": 14.27536231884058,
      "grad_norm": 3.300278663635254,
      "learning_rate": 2.4154589371980677e-06,
      "loss": 1.7404,
      "step": 3940
    },
    {
      "epoch": 14.293478260869565,
      "grad_norm": 2.970837116241455,
      "learning_rate": 2.355072463768116e-06,
      "loss": 1.8147,
      "step": 3945
    },
    {
      "epoch": 14.31159420289855,
      "grad_norm": 3.194183588027954,
      "learning_rate": 2.2946859903381644e-06,
      "loss": 1.675,
      "step": 3950
    },
    {
      "epoch": 14.329710144927537,
      "grad_norm": 3.1381900310516357,
      "learning_rate": 2.2342995169082123e-06,
      "loss": 1.7485,
      "step": 3955
    },
    {
      "epoch": 14.347826086956522,
      "grad_norm": 2.572857141494751,
      "learning_rate": 2.173913043478261e-06,
      "loss": 1.688,
      "step": 3960
    },
    {
      "epoch": 14.365942028985508,
      "grad_norm": 3.1049342155456543,
      "learning_rate": 2.1135265700483095e-06,
      "loss": 1.7844,
      "step": 3965
    },
    {
      "epoch": 14.384057971014492,
      "grad_norm": 3.1325390338897705,
      "learning_rate": 2.053140096618358e-06,
      "loss": 1.6917,
      "step": 3970
    },
    {
      "epoch": 14.402173913043478,
      "grad_norm": 3.076000213623047,
      "learning_rate": 1.9927536231884058e-06,
      "loss": 1.7048,
      "step": 3975
    },
    {
      "epoch": 14.420289855072463,
      "grad_norm": 2.739727020263672,
      "learning_rate": 1.932367149758454e-06,
      "loss": 1.7631,
      "step": 3980
    },
    {
      "epoch": 14.43840579710145,
      "grad_norm": 3.050593137741089,
      "learning_rate": 1.8719806763285025e-06,
      "loss": 1.7802,
      "step": 3985
    },
    {
      "epoch": 14.456521739130435,
      "grad_norm": 3.1491358280181885,
      "learning_rate": 1.8115942028985508e-06,
      "loss": 1.7203,
      "step": 3990
    },
    {
      "epoch": 14.47463768115942,
      "grad_norm": 2.43121075630188,
      "learning_rate": 1.7512077294685992e-06,
      "loss": 1.497,
      "step": 3995
    },
    {
      "epoch": 14.492753623188406,
      "grad_norm": 2.805398941040039,
      "learning_rate": 1.6908212560386474e-06,
      "loss": 1.7388,
      "step": 4000
    },
    {
      "epoch": 14.51086956521739,
      "grad_norm": 3.515842914581299,
      "learning_rate": 1.6304347826086957e-06,
      "loss": 1.7561,
      "step": 4005
    },
    {
      "epoch": 14.528985507246377,
      "grad_norm": 3.6826024055480957,
      "learning_rate": 1.570048309178744e-06,
      "loss": 1.7704,
      "step": 4010
    },
    {
      "epoch": 14.547101449275363,
      "grad_norm": 3.0974535942077637,
      "learning_rate": 1.5096618357487922e-06,
      "loss": 1.7532,
      "step": 4015
    },
    {
      "epoch": 14.565217391304348,
      "grad_norm": 3.785456657409668,
      "learning_rate": 1.4492753623188406e-06,
      "loss": 1.7826,
      "step": 4020
    },
    {
      "epoch": 14.583333333333334,
      "grad_norm": 2.8670101165771484,
      "learning_rate": 1.388888888888889e-06,
      "loss": 1.7592,
      "step": 4025
    },
    {
      "epoch": 14.601449275362318,
      "grad_norm": 2.844784736633301,
      "learning_rate": 1.3285024154589373e-06,
      "loss": 1.7761,
      "step": 4030
    },
    {
      "epoch": 14.619565217391305,
      "grad_norm": 2.8802783489227295,
      "learning_rate": 1.2681159420289857e-06,
      "loss": 1.7568,
      "step": 4035
    },
    {
      "epoch": 14.63768115942029,
      "grad_norm": 3.112234115600586,
      "learning_rate": 1.2077294685990338e-06,
      "loss": 1.7821,
      "step": 4040
    },
    {
      "epoch": 14.655797101449275,
      "grad_norm": 2.980731248855591,
      "learning_rate": 1.1473429951690822e-06,
      "loss": 1.747,
      "step": 4045
    },
    {
      "epoch": 14.673913043478262,
      "grad_norm": 2.8534018993377686,
      "learning_rate": 1.0869565217391306e-06,
      "loss": 1.7539,
      "step": 4050
    },
    {
      "epoch": 14.692028985507246,
      "grad_norm": 2.711376428604126,
      "learning_rate": 1.026570048309179e-06,
      "loss": 1.737,
      "step": 4055
    },
    {
      "epoch": 14.710144927536232,
      "grad_norm": 2.9938929080963135,
      "learning_rate": 9.66183574879227e-07,
      "loss": 1.7969,
      "step": 4060
    },
    {
      "epoch": 14.728260869565217,
      "grad_norm": 2.9958293437957764,
      "learning_rate": 9.057971014492754e-07,
      "loss": 1.7224,
      "step": 4065
    },
    {
      "epoch": 14.746376811594203,
      "grad_norm": 3.0288426876068115,
      "learning_rate": 8.454106280193237e-07,
      "loss": 1.7531,
      "step": 4070
    },
    {
      "epoch": 14.764492753623188,
      "grad_norm": 2.7216548919677734,
      "learning_rate": 7.85024154589372e-07,
      "loss": 1.7291,
      "step": 4075
    },
    {
      "epoch": 14.782608695652174,
      "grad_norm": 3.246173620223999,
      "learning_rate": 7.246376811594203e-07,
      "loss": 1.7056,
      "step": 4080
    },
    {
      "epoch": 14.80072463768116,
      "grad_norm": 2.9031028747558594,
      "learning_rate": 6.642512077294687e-07,
      "loss": 1.8075,
      "step": 4085
    },
    {
      "epoch": 14.818840579710145,
      "grad_norm": 3.38134765625,
      "learning_rate": 6.038647342995169e-07,
      "loss": 1.8017,
      "step": 4090
    },
    {
      "epoch": 14.83695652173913,
      "grad_norm": 3.3455820083618164,
      "learning_rate": 5.434782608695653e-07,
      "loss": 1.847,
      "step": 4095
    },
    {
      "epoch": 14.855072463768115,
      "grad_norm": 2.8622241020202637,
      "learning_rate": 4.830917874396135e-07,
      "loss": 1.7312,
      "step": 4100
    },
    {
      "epoch": 14.873188405797102,
      "grad_norm": 3.4020891189575195,
      "learning_rate": 4.2270531400966184e-07,
      "loss": 1.8282,
      "step": 4105
    },
    {
      "epoch": 14.891304347826086,
      "grad_norm": 2.7878215312957764,
      "learning_rate": 3.6231884057971015e-07,
      "loss": 1.7334,
      "step": 4110
    },
    {
      "epoch": 14.909420289855072,
      "grad_norm": 3.052762031555176,
      "learning_rate": 3.0193236714975846e-07,
      "loss": 1.7615,
      "step": 4115
    },
    {
      "epoch": 14.927536231884059,
      "grad_norm": 2.877466917037964,
      "learning_rate": 2.4154589371980677e-07,
      "loss": 1.7656,
      "step": 4120
    },
    {
      "epoch": 14.945652173913043,
      "grad_norm": 2.9469234943389893,
      "learning_rate": 1.8115942028985507e-07,
      "loss": 1.6857,
      "step": 4125
    },
    {
      "epoch": 14.96376811594203,
      "grad_norm": 3.5127065181732178,
      "learning_rate": 1.2077294685990338e-07,
      "loss": 1.8048,
      "step": 4130
    },
    {
      "epoch": 14.981884057971014,
      "grad_norm": 2.731942653656006,
      "learning_rate": 6.038647342995169e-08,
      "loss": 1.664,
      "step": 4135
    },
    {
      "epoch": 15.0,
      "grad_norm": 2.647139549255371,
      "learning_rate": 0.0,
      "loss": 1.8058,
      "step": 4140
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8147141933441162,
      "eval_runtime": 3.453,
      "eval_samples_per_second": 14.191,
      "eval_steps_per_second": 2.027,
      "step": 4140
    }
  ],
  "logging_steps": 5,
  "max_steps": 4140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1081768547450880.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
